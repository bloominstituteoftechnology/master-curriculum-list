<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>DS5 Module 4 - Logistic Regression</title>
    <link rel="stylesheet" href="../../../css/style.css" />
  </head>

  <body>
    <header>
      <nav>
        <div class="logo">Data Science Unit 2</div>
        <ul>
          <li><a href="../../index.html">Home</a></li>
          <li class="dropdown">
            <a href="#" class="active">Modules</a>
            <div class="dropdown-content">
              <a href="../module1/index.html">Module 1: Linear Regression 1</a>
              <a href="../module2/index.html">Module 2: Linear Regression 2</a>
              <a href="../module3/index.html">Module 3: Ridge Regression</a>
              <a href="../module4/index.html" class="active"
                >Module 4: Logistic Regression</a
              >
            </div>
          </li>
          <li><a href="../../code-alongs/index.html">Code-Alongs</a></li>
          <li>
            <a href="../../sprint-challenge/index.html">Sprint Challenge</a>
          </li>
        </ul>
      </nav>
    </header>

    <main class="container">
      <h1>Module 4: Logistic Regression</h1>

      <section class="content-box">
        <h2>Module Overview</h2>
        <p>
          In this module, you will transition from regression to classification
          with logistic regression. You'll implement train-validate-test splits,
          understand classification baselines, and learn about scikit-learn
          pipelines. These skills will enable you to build and evaluate models
          for binary classification problems.
        </p>
      </section>

      <section class="content-box">
        <h2>Learning Objectives</h2>
        <ul>
          <li>Implement a train-validate-test split</li>
          <li>Begin with baselines for classification</li>
          <li>
            Express and explain the intuition and interpretation of logistic
            regression
          </li>
          <li>
            Use scikit-learn to fit and interpret logistic regression models
          </li>
          <li>Use scikit-learn pipelines</li>
        </ul>
      </section>

      <section class="content-box">
        <h2>Objective 01 - Implement a train-validate-test split</h2>
        <h3>Overview</h3>
        <p>
          In the previous module we used a train-test split, where we hold back
          a subset of the data to use for testing the model. When we train a
          model we also need to evaluate the model. Recall that if we evaluate
          on the training data we're not getting an accurate estimate of the
          true performance of the model. For this reason, we need to use test
          data that the model has not yet seen.
        </p>
        <p>
          Sometimes it's useful to be able to have an intermediate step where
          the model can be evaluated without using the set-aside test set. This
          is where a validation set is useful. Consider the situation where we
          take a subset of our data and set it aside as the test set - we won't
          touch this data until we're ready to evaluate a final model.
        </p>
        <p>
          With the remaining data, we can divide it into training and validation
          sets. We then train the model on the training data and evaluate it on
          the validation data. Another advantage of using a validation set is
          that it can be used to tune the model or adjust the hyperparameters.
          Iterations of tuning and model fitting are used to find the final
          model, which is then evaluated using the test set.
        </p>
        <h3>Train-validate-test</h3>
        <p>Some general definitions are:</p>
        <ul>
          <li>
            <strong>training dataset</strong>: the sample of data used to fit
            the model
          </li>
          <li>
            <strong>validation dataset</strong>: the sample of data used to
            evaluate the model and possibly to adjust the hyperparameters
          </li>
          <li>
            <strong>testing dataset</strong>: the sample of data used for final
            model testing; not to be used for anything other than testing so
            that the result is unbiased
          </li>
        </ul>
        <p>
          One last point to make is that sometimes you won't even have access
          the test set! If you are participating in a Kaggle competition, for
          example, you cannot view the actual target values for the test data,
          and can only generate the predictions for your submission. The number
          of test prediction submissions might be limited, or you might not want
          to make numerous test submissions just to evaluate or tune your model.
        </p>
        <p>
          In this next section, we'll create our own train-validation-test data
          sets. We'll follow the guideline of using 60% for training, 20% for
          validation, and 20% for testing.
        </p>
        <h3>Follow Along</h3>
        <p>
          We haven't yet worked with the Iris dataset in this module, so we'll
          start there. In the following example, we load the data and then
          separate out the feature <code>petal_width</code> and the target
          <code>petal_length</code>. Having plotted this data earlier, we know
          there is a linear relationship between the petal width and length: the
          wider the petal, the greater the length. We'll use a linear model to
          predict our target.
        </p>
        <pre><code># Import numpy and seaborn
import numpy as np
import seaborn as sns

iris = sns.load_dataset("iris")
display(iris.head())

x = iris['petal_width'] 
X = np.array(x)[:, np.newaxis]
y = iris['petal_length']
</code></pre>
        <div class="table-responsive">
          <table class="custom-table">
            <thead>
              <tr>
                <th></th>
                <th>sepal_length</th>
                <th>sepal_width</th>
                <th>petal_length</th>
                <th>petal_width</th>
                <th>species</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>0</td>
                <td>5.1</td>
                <td>3.5</td>
                <td>1.4</td>
                <td>0.2</td>
                <td>setosa</td>
              </tr>
              <tr>
                <td>1</td>
                <td>4.9</td>
                <td>3.0</td>
                <td>1.4</td>
                <td>0.2</td>
                <td>setosa</td>
              </tr>
              <tr>
                <td>2</td>
                <td>4.7</td>
                <td>3.2</td>
                <td>1.3</td>
                <td>0.2</td>
                <td>setosa</td>
              </tr>
              <tr>
                <td>3</td>
                <td>4.6</td>
                <td>3.1</td>
                <td>1.5</td>
                <td>0.2</td>
                <td>setosa</td>
              </tr>
              <tr>
                <td>4</td>
                <td>5.0</td>
                <td>3.6</td>
                <td>1.4</td>
                <td>0.2</td>
                <td>setosa</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>
          First, we'll hold back a subset of the data just for the test data.
          We'll do this with the scikit-learn utility. We'll call it something
          different from "train" so that we don't confuse it with the actual
          training data later.
        </p>
        <pre><code># Import the train_test_split utility
from sklearn.model_selection import train_test_split

# Create the "remaining" and test datasets
X_remain, X_test, y_remain, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)
</code></pre>
        <p>
          Then we'll create a training set and validation set from the remaining
          data. We could have done this in one step but we're breaking it down
          here so it's easier to see that we removed a test subset and will not
          accidentally use it for evaluation until we're ready to test.
        </p>
        <pre><code># Create the train and validation datasets

X_train, X_val, y_train, y_val = train_test_split(
    X_remain, y_remain, test_size=0.25, random_state=42)

# Print out sizes of train, validate, test datasets

print('Training data set samples:', len(X_train))
print('Validation data set samples:', len(X_val))
print('Test data set samples:', len(X_test))
</code></pre>
        <pre><code>Training data set samples: 90
Validation data set samples: 30
Test data set samples: 30
</code></pre>
        <p>Now we can fit our model and evaluate it on our validation set.</p>
        <pre><code># Import the predictor and instantiate the class
from sklearn.linear_model import LinearRegression

# Instantiate the model
model = LinearRegression()

# Fit the model
model.fit(X_train, y_train)

# Use the VALIDATION set for prediction
y_predict = model.predict(X_val)

# Calculate the accuracy score
from sklearn.metrics import r2_score
r2_score(y_val, y_predict)
</code></pre>
        <pre><code>0.9589442606386026
</code></pre>
        <p>
          Well, that's a pretty good model score (R-squared), which we expect
          because we know the Iris dataset has a strong linear trend between the
          petal width and petal length. Now would be the time to change any of
          the model hyperparameters and evaluate on the validation set again.
          We'll continue with the default model parameters for now.
          Hyperparameter tuning is something that will be introduced in the
          later Sprints.
        </p>
        <p>Now, let's use the test set we held back above.</p>
        <pre><code># Use the TEST set for prediction
y_predict_test = model.predict(X_test)

# Calculate the accuracy score

r2_score(y_test, y_predict_test)
</code></pre>
        <pre><code>0.9287783612248339
</code></pre>
        <p>
          The R-squared score is a little lower than it was for the validate
          set. If we were to run the model and test again with a different
          random seed, the scores would be different and the test score might be
          higher.
        </p>
        <h3>Challenge</h3>
        <p>
          Using the same data set as in the example, try changing the
          <code>random_state</code> parameter to see how the validate and test
          model scores change.
        </p>
        <h3>Additional Resources</h3>
        <ul>
          <li>
            <a
              href="https://machinelearningmastery.com/difference-test-validation-datasets/"
              target="_blank"
              rel="noopener noreferrer"
            >
              What is the Difference Between Test and Validation Datasets?<!--Links to an external site.-->
            </a>
          </li>
        </ul>
      </section>

      <section class="content-box"></section>

      <section class="content-box"></section>

      <section class="content-box"></section>

      <section class="content-box"></section>

      <section class="content-box"></section>

      <section class="content-box"></section>

      <section class="content-box">
        <h2>Guided Project</h2>
        <p>
          Open <strong>JDS_SHR_214_guided_project_notes.ipynb</strong> in the
          GitHub repository below to follow along with the guided project:
        </p>
        <div class="resource-links">
          <a
            href="https://github.com/bloominstituteoftechnology/DS-Unit-2-Linear-Models/tree/master/module4-logistic-regression"
            class="resource-link"
            target="_blank"
            rel="noopener noreferrer"
            >GitHub: Logistic Regression</a
          >
          <a
            href="https://docs.google.com/presentation/d/1VWvH9jKBj63sirqUsQuNxdNme97rEO5uElH8AlBdbcM/present?slide=id.g125f5691cb9_0_0"
            class="resource-link"
            target="_blank"
            rel="noopener noreferrer"
            >Slides</a
          >
        </div>

        <h2>Guided Project Video</h2>
        <div class="video-container">
          <iframe
            class="wistia_embed"
            title="Sprint 5 Logistic Regression Video"
            src="https://fast.wistia.net/embed/iframe/rlh3upen6m"
            width="640"
            height="360"
            allow="fullscreen"
            loading="lazy"
          ></iframe>
        </div>
      </section>

      <section class="content-box">
        <h2>Module Assignment</h2>
        <p>
          Complete the Module 4 assignment to practice logistic regression
          techniques you've learned.
        </p>
        <div class="resource-links">
          <a
            href="https://github.com/bloominstituteoftechnology/DS-Unit-2-Linear-Models/blob/master/module4-logistic-regression/LS_DS_214_assignment.ipynb"
            class="resource-link"
            target="_blank"
            rel="noopener noreferrer"
            >Module 4 Assignment</a
          >
          <a
            href="https://srcole.github.io/100burritos/"
            class="resource-link"
            target="_blank"
            rel="noopener noreferrer"
            >Dataset: Burritos of San Diego</a
          >
        </div>

        <h2>Assignment Solution Video</h2>
        <div class="video-container">
          <iframe
            class="wistia_embed"
            title="Logistic Regression - Module Project Solution Video"
            src="https://fast.wistia.net/embed/iframe/8kq0iynda6"
            width="640"
            height="360"
            allow="fullscreen"
            loading="lazy"
          ></iframe>
        </div>
      </section>

      <section class="content-box">
        <h2>Resources</h2>

        <h3>Documentation and Tutorials</h3>
        <ul>
          <li>
            <a
              href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
              target="_blank"
              rel="noopener noreferrer"
              >Scikit-learn: train_test_split</a
            >
          </li>
          <li>
            <a
              href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
              target="_blank"
              rel="noopener noreferrer"
              >Scikit-learn: LogisticRegression</a
            >
          </li>
          <li>
            <a
              href="https://scikit-learn.org/stable/modules/compose.html#pipeline"
              target="_blank"
              rel="noopener noreferrer"
              >Scikit-learn: Pipelines</a
            >
          </li>
          <li>
            <a
              href="https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics"
              target="_blank"
              rel="noopener noreferrer"
              >Scikit-learn: Classification Metrics</a
            >
          </li>
        </ul>

        <h3>Articles and Readings</h3>
        <ul>
          <li>
            <a
              href="https://machinelearningmastery.com/logistic-regression-for-machine-learning/"
              target="_blank"
              rel="noopener noreferrer"
              >Logistic Regression for Machine Learning</a
            >
          </li>
          <li>
            <a
              href="https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7"
              target="_blank"
              rel="noopener noreferrer"
              >Your validation loss is lower than your training loss? This is
              why!</a
            >
          </li>
        </ul>
      </section>
    </main>
  </body>
</html>
