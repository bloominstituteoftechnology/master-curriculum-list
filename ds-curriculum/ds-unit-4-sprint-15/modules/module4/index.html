<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 4: Large Language Models</title>
    <link rel="stylesheet" href="../../../css/style.css">
</head>

<body>
    <header>
        <nav>
            <div class="logo">Data Science Unit 4</div>
            <ul>
                <li><a href="../../index.html">Home</a></li>
                <li class="dropdown">
                    <a href="#" class="active">Modules</a>
                    <div class="dropdown-content">
                        <a href="../module1/index.html">Module 1: Recurrent Neural Networks and LSTM</a>
                        <a href="../module2/index.html">Module 2: Convolutional Neural Networks</a>
                        <a href="../module3/index.html">Module 3: OpenAI and ChatGPT</a>
                        <a href="../module4/index.html" class="active">Module 4: Large Language Models</a>
                    </div>
                </li>
                <li><a href="../../code-alongs/index.html">Code-Alongs</a></li>
                <li><a href="../../sprint-challenge/index.html">Sprint Challenge</a></li>
            </ul>
        </nav>
    </header>

    <main class="container">
        <section id="welcome">
            <h1>Module 4: Large Language Models</h1>
            <div class="content-box">
                <h2>Module Overview</h2>
                <p>This updated module takes you beyond interacting with existing LLM interfaces to building and
                    customizing your own LLM-powered applications. Building on the foundation from Module 3, you'll
                    learn to work directly with LLM APIs and local models to create sophisticated, context-aware
                    conversational agents.</p>

                <p>You'll explore how to design and implement local LLM bots with customizable prompts and parameters,
                    experiment with different model configurations, and tackle advanced challenges like implementing
                    memory systems for more coherent conversations. This hands-on approach will give you practical
                    experience in building production-ready LLM applications while understanding the technical
                    considerations involved in deploying these powerful models.</p>
            </div>
        </section>

        <section id="learning-objectives">
            <div class="content-box">
                <h2>Learning Objectives</h2>
                <ul>
                    <li>Develop and customize local LLM bots with parameterized prompts and configurations</li>
                    <li>Implement memory systems and context management for enhanced conversational experiences</li>
                </ul>
            </div>
        </section>

        <section class="content-box">
            <h2>Objective 01 - OpenAI API - The API Behind ChatGPT</h2>
            <h3>API Basics</h3>
            <p>
                In the digital landscape, software applications often possess unique functionalities that can be
                beneficial when integrated into other applications. A special interface is designed to handle external
                requests from other applications to facilitate such feature-sharing. This interface, known as the
                Application Programming Interface (API), serves as a secure and streamlined conduit for sharing
                capabilities and data between different software solutions. By utilizing an API, developers can enrich
                their own applications with the functionalities of another, amplifying the utility and reach of both
                platforms.
            </p>
            <h3>What is OpenAI's API and SDK?</h3>
            <p>
                An API serves as a bridge that allows two different applications to communicate with each other. In the
                context of OpenAI's GPT, the API provides a way to programmatically interact with OpenAI's models for
                various tasks like text generation, summarization, and more. From scratch, APIs can be involved, but
                with the use of tools like Software Development Kits, you can speed up the process.
            </p>
            <p>
                Software Development Kits (SDKs) are collections of software tools and libraries that simplify complex
                actions, making it easier to interact with an API. OpenAI provides an SDK for Python that wraps the raw
                API calls, offering a more Pythonic way to make requests to the GPT models.
            </p>
            <h3>Setting Up OpenAI Python SDK</h3>
            <p>To work with the OpenAI Python SDK, you'll need:</p>
            <ul>
                <li>Python 3.6 or higher</li>
                <li>OpenAI account</li>
                <li>API key from OpenAI</li>
            </ul>
            <h4>Installation</h4>
            <p>Installing the OpenAI Python SDK is simple. Use pip to install the package:</p>
            <pre><code>pip install openai</code></pre>
            <h4>API Key Configuration</h4>
            <p>
                After obtaining your API key from OpenAI, you can set it up in one of two ways:
            </p>
            <ul>
                <li><strong>Environment Variable:</strong> Set an environment variable called
                    <code>OPENAI_API_KEY</code> with the key as its value.
                </li>
                <li><strong>Directly in Code:</strong> Pass the API key as an argument while initializing the SDK.</li>
            </ul>
            <h3>Making First SDK Call</h3>
            <h4>Initializing SDK</h4>
            <p>
                To use the OpenAI SDK in your Python code, you need to import it and initialize it with your API key.
            </p>
            <pre><code>import os

from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))</code></pre>
            <h4>Simple Example: Text Generation</h4>
            <p>Here's a quick example to generate text using the GPT-3 model via the OpenAI SDK:</p>
            <pre><code>text = "Welcome to Data Science"
response = client.completions.create(
    model="gpt-3.5-turbo-instruct",
    prompt=f"Translate the following English text to French: '{text}'",
    max_tokens=60
)
generated_text = response.choices[0].text.strip()
print(generated_text)
</code></pre>
            <p>
                In this example, the text generated will be the French translation of the English text specified in the
                <code>text</code> variable.
            </p>
        </section>

        <section class="content-box">
            <h2>Objective 02 - OpenAI API SDK</h2>
            <h3>OpenAI API SDK</h3>
            <h4>Introduction</h4>
            <p>
                The OpenAI API SDK serves as a powerful tool for leveraging machine learning capabilities within your
                applications. By manipulating parameters such as <code>prompt</code>, <code>max_tokens</code>,
                <code>temperature</code>, and <code>top_p</code>, users can fine-tune the output to meet their specific
                needs. Whether you are aiming for more deterministic results or embracing randomness, understanding
                these options enables you to exploit the full potential of the API.
            </p>
            <h4>prompt</h4>
            <p>
                The <code>prompt</code> parameter is the initial string that guides the model in generating a
                completion. The more specific and contextual the prompt, the more accurate the generated text will be.
            </p>
            <h4>max_tokens</h4>
            <p>
                This parameter limits the number of tokens in the output. If you set <code>max_tokens</code> to 50, the
                model will generate text up to 50 tokens long.
            </p>
            <h4>temperature</h4>
            <p>
                The <code>temperature</code> parameter controls the randomness of the output. A higher value like 0.8
                yields more random outputs, while a lower value like 0.2 makes the output more deterministic.
            </p>
            <h4>top_p</h4>
            <p>
                This parameter controls the nucleus sampling, which filters the token pool before choosing the next
                token. Values are between 0 and 1; lower values make the text more focused and deterministic.
            </p>
            <h3>Handling SDK Responses</h3>
            <h4>Response Object</h4>
            <p>
                When you make an API call using the SDK, you receive a response object. This object contains various
                pieces of information, including the generated text.
            </p>
            <p>
                To extract the generated text from the response object, you can use the following code snippet:
            </p>
            <pre><code>import os

from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

text = "Welcome to Data Science"
response = client.completions.create(
    model="gpt-3.5-turbo-instruct",
    prompt=f"Translate the following English text to French: '{text}'",
    max_tokens=60
)

generated_text = response.choices[0].text.strip()
print(generated_text)
</code></pre>
            <p>As a function...</p>
            <pre><code>def extract_reply(response):
    return response.choices[0].message.content.strip()
</code></pre>
            <h4>Error Handling</h4>
            <p>
                To handle errors gracefully, you can use Python's try-except blocks. Here's an example:
            </p>
            <pre><code>import os

from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

try:
    text = "Welcome to Data Science"
    response = client.completions.create(
        model="gpt-3.5-turbo-instruct",
        prompt=f"Translate the following English text to French: '{text}'",
        max_tokens=60
    )
    
    generated_text = response.choices[0].text.strip()
    print(generated_text)
except Exception as e:
    print(f"An error occurred: {e}")
</code></pre>
            <p>
                You may need to account for more common errors; errors you might find in any API. Here is the short list
                of those errors you may encounter:
            </p>
            <ul>
                <li><strong>RateLimitExceeded:</strong> You've exceeded the number of requests permitted in a given time
                    frame.</li>
                <li><strong>ResourceNotFound:</strong> The engine specified does not exist.</li>
                <li><strong>InvalidRequestError:</strong> The API request was malformed.</li>
            </ul>
            <h3>Advanced SDK Usage</h3>
            <h4>Batching Requests</h4>
            <p>
                To make multiple requests at once, you can use the SDK's batch support. This is more efficient than
                making individual calls.
            </p>
            <h4>Pagination</h4>
            <p>
                When dealing with a large amount of generated text, you can paginate the results. This helps in managing
                the tokens effectively.
            </p>
            <h3>Deep Dive: System Prompts in OpenAI's GPT API with Python SDK</h3>
            <p>
                System prompts are special instructions given to the model to guide its behavior throughout an
                interactive session or for a specific task. These are often used in conversational agents, content
                filters, and other scenarios where you need to condition the model's responses according to specific
                guidelines or goals.
            </p>
            <h4>Types of System Prompts</h4>
            <ul>
                <li><strong>Conversational Directives</strong><br>
                    You can use system prompts to instruct the model to behave like a specific character or to adopt a
                    particular tone, style, or point of view. For example, instructing the model to speak like
                    Shakespeare or to adopt a formal tone.
                </li>
                <li><strong>Content Filtering</strong><br>
                    System prompts can also be used to enforce ethical guidelines, like avoiding generating harmful or
                    inappropriate content.
                </li>
                <li><strong>Task-Specific Instructions</strong><br>
                    For specialized tasks like code generation, data analysis, or text summarization, system prompts can
                    provide high-level directives that guide the model's behavior throughout the session.
                </li>
            </ul>
            <h4>Format of System Prompts</h4>
            <p>
                The system prompt is generally set up at the beginning of an interaction and stays consistent
                throughout. It's often placed at the top of the prompt string, separate from user or task-specific
                prompts, to provide a general context or instruction set for the model.
            </p>
            <h4>Implementing System Prompts with SDK</h4>
            <p>
                Here's how you can include a system prompt while generating text using the chat interface:
            </p>
            <pre><code>import os

from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))


def extract_reply(response):
    return response.choices[0].message.content.strip()


messages = [
    {"role": "system", "content": "You are an assistant that speaks like Shakespeare."},
    {"role": "user", "content": "How is the weather today?"}
]

result = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=messages,
)

reply = extract_reply(result)
print(reply)
</code></pre>
            <h4>Considerations for Using System Prompts</h4>
            <ul>
                <li><strong>Token Limit:</strong> Remember that system prompts consume tokens, so be mindful of the
                    <code>max_tokens</code> parameter to ensure the output is not truncated.
                </li>
                <li><strong>Prompt Clarity:</strong> The clearer and more specific your system prompt, the better the
                    model will be at following the guidelines or rules you've set.</li>
                <li><strong>Testing:</strong> It's crucial to test the effectiveness of a system prompt rigorously to
                    ensure it guides the model's behavior as intended.</li>
            </ul>
            <p>
                By mastering the use of system prompts, you can make the most out of OpenAI's GPT API and Python SDK for
                a wide array of specialized and interactive tasks.
            </p>
            <h3>Conclusion</h3>
            <p>
                In this module, we delved into the OpenAI Python SDK as a powerful tool for interacting with GPT models.
                Starting from the basic setup requirements and installation, we progressed through the various
                parameters like <code>prompt</code>, <code>max_tokens</code>, <code>temperature</code>, and
                <code>top_p</code> that help fine-tune the behavior and output of the GPT model.
            </p>
            <p>
                We also took a deep dive into the concept of system prompts, a versatile feature that allows you to
                guide the model's behavior for specialized tasks, enforce ethical guidelines, or add a conversational
                context. Whether you're building a conversational agent, a content filter, or a specialized text
                generator, understanding how to effectively utilize system prompts can be a game-changer.
            </p>
            <p>
                As you move forward, remember that the key to effectively using the API and SDK lies in your
                understanding of these parameters and features. Each project may require a different combination of
                them, so it's important to experiment and find what works best for your specific needs.
            </p>
            <p>
                With this foundation, you are well-prepared to explore more advanced topics and applications in future
                modules.
            </p>
            <h3>Additional Resources</h3>
            <ul>
                <li><a href="https://platform.openai.com/docs/api-reference" target="_blank"
                        rel="noopener noreferrer">OpenAI API Reference</a></li>
            </ul>
        </section>

        <section class="content-box">
            <h2>Objective 03 - Local LLM Setup</h2>
            <h3>Introduction</h3>
            <p>
                This module outlines the setup and usage of a Local Language Model (LLM) to create a chatbot named Marv,
                who is programmed to provide sarcastic responses. The LLM is powered by the <code>llama_cpp</code>
                Python package and is fine-tuned to answer queries based on the persona set by the system prompt.
            </p>
            <h3>Topics Covered</h3>
            <ul>
                <li>Installing Dependencies</li>
                <li>Initializing the LLM</li>
                <li>Crafting System and User Prompts</li>
                <li>Running the LLM and Obtaining a Response</li>
                <li>Understanding the Parameters</li>
            </ul>
            <h3>Installing Dependencies</h3>
            <p>
                To get started, install the <code>llama-cpp-python</code> package using pip.
            </p>
            <pre><code>pip install llama-cpp-python</code></pre>
            <h4>Download the LLM</h4>
            <ul>
                <li><a href="https://huggingface.co/TheBloke/OpenOrca-Platypus2-13B-GGUF/resolve/main/openorca-platypus2-13b.Q4_K_M.gguf"
                        target="_blank" rel="noopener noreferrer">openorca-platypus2</a></li>
            </ul>
            <h3>Initializing the LLM</h3>
            <p>
                Import the <code>Llama</code> class and initialize it with the appropriate model path.
            </p>
            <pre><code>from llama_cpp import Llama
llm = Llama(model_path="./app/models/openorca-platypus2-13b.Q4_K_M.gguf")</code></pre>
            <h3>Crafting System and User Prompts</h3>
            <p>
                Set up the system and user prompts. The system prompt acts as the instruction for the LLM, specifying
                its persona. The user prompt serves as the query or statement from the user.
            </p>
            <ul>
                <li>
                    <strong>System Prompt:</strong> It serves as the instruction for the model, defining its persona. In
                    this example, the persona is "Marv, a chatbot that reluctantly answers questions with sarcastic
                    responses." This instructs the model to generate replies that are sarcastic in nature. The system
                    prompt is often crucial in setting the tone, style, and context for how the language model should
                    behave.
                </li>
                <li>
                    <strong>User Prompt:</strong> This is the query or statement from the user. The language model takes
                    this as the actual question or issue to respond to. In your example, the user prompt is "In the
                    coming months AI will," which would be a starting point for the model to generate a continuation.
                </li>
            </ul>
            <pre><code>system_prompt = "You are Marv, a chatbot that reluctantly answers questions with sarcastic responses."
user_prompt = "Hi Marv, what's up?"</code></pre>
            <h3>Running the LLM and Obtaining a Response</h3>
            <p>
                Create a composite prompt by combining the system and user prompts and run the LLM. Extract and print
                the response.
            </p>
            <pre><code>prompt = f"### Instruction: {system_prompt}\n\n{user_prompt}\n\n### Response:\n"
raw_output = llm(prompt, stop=["###"], max_tokens=-1, temperature=1)
reply = raw_output.get("choices")[0].get("text").strip()
print(reply)
</code></pre>
            <h3>Understanding the Parameters</h3>
            <ul>
                <li><code>stop=["###"]</code>: Stops token generation at "###".</li>
                <li><code>max_tokens</code>: Sets a limit on the number of tokens. -1 for no limit.</li>
                <li><code>temperature</code>: Controls the randomness of output, ranging from 0 to 1.</li>
            </ul>
            <p>
                This module equips you with the know-how to set up and run a sarcastic chatbot using a Local Language
                Model. Feel free to modify the prompts and parameters as needed.
            </p>
            <h3>Putting It All together: Marv the Sarcastic Bot</h3>
            <pre><code>from llama_cpp import Llama

system_prompt = "You are Marv, a chatbot that reluctantly answers " \
                "questions with sarcastic responses."

user_prompt = "Hi Marv, what's up?"

prompt = f"### Instruction: {system_prompt}\n\n" \
         f"{user_prompt}\n\n" \
         f"### Response:\n"

llm = Llama(model_path="./app/models/openorca-platypus2-13b.Q4_K_M.gguf")

raw_output = llm(
    prompt,
    stop=["###"],
    max_tokens=-1,
    temperature=1,
)

reply = raw_output.get("choices")[0].get("text").strip()
print(reply)
</code></pre>
            <h3>Additional Resources</h3>
            <ul>
                <li><a href="https://huggingface.co/models?pipeline_tag=text-generation&sort=trending" target="_blank"
                        rel="noopener noreferrer">Huggingface Text Generation Models</a></li>
            </ul>
        </section>

        <section id="guided-project">
            <div class="content-box">
                <h2>Guided Project</h2>

                <p>This guided project focuses on hands-on LLM implementation and does not have traditional repository
                    materials. For students interested in exploring additional technical background, you can review the
                    <a href="https://github.com/bloominstituteoftechnology/DS-Unit-4-Sprint-3-Deep-Learning/tree/main/module4-time-series"
                        target="_blank" rel="noopener noreferrer">legacy Time Series Forecasting material</a> as
                    supplementary
                    content, though the current guided project and assignment are the primary focus.
                </p>

                <h3>Building a Chatbot with Persitent Memory</h3>

                <div class="video-container">
                    <iframe class="wistia_embed" title="Working with Large Language Models Video"
                        src="https://fast.wistia.net/embed/iframe/ent2k5vi8x?seo=false&videoFoam=false" width="640"
                        height="360" name="wistia_embed" allow="fullscreen" loading="lazy"></iframe>
                </div>
            </div>
        </section>

        <section id="module-assignment">
            <div class="content-box">
                <h2>Module Assignment</h2>
                <p>This module features a hands-on implementation assignment that differs from our typical structured
                    exercises.</p>

                <h3>Building an Advanced Local LLM Bot</h3>

                <h4>Objective:</h4>
                <p>The main goal of this assignment is to develop a local LLM bot with customizable prompts and
                    parameters. As a stretch goal, you will implement a short-term memory model for the bot, allowing
                    for more coherent and context-aware interactions.</p>

                <p>The instructions for this project are intentionally a bit vague. The purpose of this is for you to
                    build something of your own design, which can present many challenges and more importantly, a
                    portfolio-worthy project.</p>

                <h4>Prerequisites:</h4>
                <ul>
                    <li>Python programming experience</li>
                    <li>Basic understanding of machine learning, NLP, and LLMs</li>
                    <li>Access to an LLM API or local LLM setup</li>
                </ul>

                <h4>Steps:</h4>
                <ol>
                    <li><strong>Initial Setup</strong>
                        <ul>
                            <li>Set up a basic bot using a local LLM or an API service.</li>
                        </ul>
                    </li>
                    <li><strong>Experimentation</strong>
                        <ul>
                            <li>Experiment with various prompts and parameters to understand their impact on the bot's
                                responses.</li>
                        </ul>
                    </li>
                    <li><strong>Refactoring</strong>
                        <ul>
                            <li>Refactor your bot into a function or class, making sure to parameterize the user_prompt.
                            </li>
                        </ul>
                    </li>
                    <li><strong>Memory Module (Stretch Goal)</strong>
                        <ul>
                            <li>Implement a memory system for your bot. This can range from simply feeding back previous
                                interactions to a more complex approach like a vector database for automatic relevant
                                recall.</li>
                        </ul>
                    </li>
                    <li><strong>Evaluation</strong>
                        <ul>
                            <li>Evaluate the performance in terms of coherence, relevance, and context-awareness.</li>
                        </ul>
                    </li>
                    <li><strong>Documentation</strong>
                        <ul>
                            <li>Document your design choices, implementation details, and observations.</li>
                        </ul>
                    </li>
                    <li><strong>Peer Review (Stretch Goal)</strong>
                        <ul>
                            <li>Share your project for peer review, focusing on the bot's design, performance, and
                                memory model.</li>
                        </ul>
                    </li>
                    <li><strong>Final Submission</strong>
                        <ul>
                            <li>Submit your code and documentation for evaluation.</li>
                        </ul>
                    </li>
                </ol>

                <h4>Evaluation Criteria:</h4>
                <ul>
                    <li>Quality of the design and implementation of the bot</li>
                    <li>Effectiveness of the parameterization and customization</li>
                    <li>Implementation and performance of the memory model (if attempted)</li>
                    <li>Peer review feedback (optional)</li>
                </ul>

                <h4>Resources:</h4>
                <ul>
                    <li><a href="https://platform.openai.com/docs/guides/text?api-mode=chat" target="_blank"
                            rel="noopener noreferrer">OpenAI API</a></li>
                    <li><a href="https://huggingface.co/models?pipeline_tag=text-generation&sort=trending"
                            target="_blank" rel="noopener noreferrer">Hugging Face LLMs</a></li>
                </ul>

                <h3>Assignment Solution Video</h3>
                <div class="video-container">
                    <iframe class="wistia_embed" title="LLM Bot Assignment Solution Video"
                        src="https://fast.wistia.net/embed/iframe/8psbb0unf3?seo=false&videoFoam=false" width="640"
                        height="360" name="wistia_embed" allow="fullscreen" loading="lazy"></iframe>
                </div>
            </div>
        </section>

        <section id="additional-resources">
            <div class="content-box">
                <h2>Additional Resources</h2>
                <h3>LLM APIs and Platforms</h3>
                <ul>
                    <li><a href="https://platform.openai.com/docs/api-reference" target="_blank"
                            rel="noopener noreferrer">OpenAI
                            API Reference</a></li>
                    <li><a href="https://huggingface.co/docs/transformers/main_classes/pipelines" target="_blank"
                            rel="noopener noreferrer">Hugging Face Transformers Pipelines</a></li>
                    <li><a href="https://docs.anthropic.com/claude/docs" target="_blank"
                            rel="noopener noreferrer">Anthropic Claude
                            API Documentation</a></li>
                    <li><a href="https://ai.google.dev/docs" target="_blank" rel="noopener noreferrer">Google AI
                            Platform
                            Documentation</a></li>
                </ul>
                <h3>Local LLM Implementation</h3>
                <ul>
                    <li><a href="https://ollama.ai/" target="_blank" rel="noopener noreferrer">Ollama: Run LLMs
                            Locally</a></li>
                    <li><a href="https://github.com/ggerganov/llama.cpp" target="_blank"
                            rel="noopener noreferrer">llama.cpp:
                            Efficient LLM Inference</a></li>
                    <li><a href="https://docs.llamaindex.ai/en/stable/" target="_blank"
                            rel="noopener noreferrer">LlamaIndex: Data
                            Framework for LLMs</a></li>
                    <li><a href="https://python.langchain.com/docs/get_started/introduction" target="_blank"
                            rel="noopener noreferrer">LangChain: Building Applications with LLMs</a></li>
                </ul>
                <h3>Memory and Context Management</h3>
                <ul>
                    <li><a href="https://github.com/chroma-core/chroma" target="_blank"
                            rel="noopener noreferrer">Chroma: Vector
                            Database for LLMs</a></li>
                    <li><a href="https://www.pinecone.io/learn/vector-database/" target="_blank"
                            rel="noopener noreferrer">Pinecone: Vector Database Guide</a></li>
                    <li><a href="https://python.langchain.com/docs/how_to/chatbots_memory/" target="_blank"
                            rel="noopener noreferrer">LangChain: How to Add Memory to Chatbots</a></li>
                </ul>
            </div>
        </section>
    </main>
</body>

</html>