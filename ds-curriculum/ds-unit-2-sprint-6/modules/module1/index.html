<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>DS6 Module 1 - Decision Trees</title>
    <link rel="stylesheet" href="../../../css/style.css" />
</head>

<body>
    <header>
        <nav>
            <div class="logo">Data Science Unit 2</div>
            <ul>
                <li><a href="../../index.html">Home</a></li>
                <li class="dropdown">
                    <a href="#" class="active">Modules</a>
                    <div class="dropdown-content">
                        <a href="../module1/index.html" class="active">Module 1: Decision Trees</a>
                        <a href="../module2/index.html">Module 2: Random Forests</a>
                        <a href="../module3/index.html">Module 3: Cross-Validation and Grid Search</a>
                        <a href="../module4/index.html">Module 4: Classification Metrics</a>
                    </div>
                </li>
                <li><a href="../../code-alongs/index.html">Code-Alongs</a></li>
                <li>
                    <a href="../../sprint-challenge/index.html">Sprint Challenge</a>
                </li>
            </ul>
        </nav>
    </header>

    <main class="container">
        <h1>Module 1: Decision Trees</h1>

        <section class="content-box">
            <h2>Module Overview</h2>
            <p>
                In this module, you will learn about decision trees, one of the most
                intuitive and widely used machine learning algorithms. Decision trees
                are versatile models that can be used for both classification and
                regression tasks, making them essential tools in a data scientist's
                toolkit.
            </p>
            <p>
                You'll explore how decision trees work, from the basic concepts of
                node splitting to practical implementation using scikit-learn. You'll
                also learn about the strengths and limitations of decision trees,
                setting the foundation for more advanced tree-based methods in later
                modules.
            </p>
        </section>

        <section class="content-box">
            <h2>Learning Objectives</h2>
            <ul>
                <li>Clean data with outliers and missing values</li>
                <li>Build a Decision Tree using scikit-learn</li>
                <li>Get and interpret feature importances of a tree-based model</li>
            </ul>
        </section>

        <section class="content-box">
            <h2>Objective 01 - clean data with outliers and missing values</h2>
            <h3>Overview</h3>
            <p>
                Cleaning data is an important task when we are beginning any machine learning or data science project.
                Real-world data is often messy, with missing values, outliers, incorrect values, and many other issues.
                In this objective we're going to focus on two of the more common problems: outliers and missing values.
            </p>
            <p>
                An outlier is a data point that is different from the rest of your data set in some way. It can be the
                result of an error, corrupted data, or a true value that just happens to be different from the rest.
                When dealing with outliers we first need to identify them and then remove or adjust them. It's important
                to do this because model training and performance can be affected by outliers, especially if you don't
                know what type or how many are present in your data.
            </p>
            <h3>Identifying Outliers</h3>
            <p>
                There are a number of different methods used to identify outliers. In this objective we're not going to
                go into great detail because these methods depend on the type of data you are working with and also the
                type of model(s) you are trying to fit. There are however a few basic methods that generally apply when
                detecting outliers. One of the most common is to perform a statistical analysis to identify data points
                that are statistically different from the rest. For example, you might assume your data is normally
                distributed and look for values that are a certain distance from the mean. Extreme values are easy to
                find and filter out this way if needed.
            </p>
            <p>
                Another method is to just look at the data. This is something you have likely already done - visualizing
                your data is an important first step. Depending on the type of data you are working with, a scatter plot
                or histogram are useful tools to help identify outliers. Some types of plots, such as boxplots, are also
                a great way to visualize the descriptive statistics of a data set.
            </p>
            <h3>Handling Outliers</h3>
            <p>
                Once you identify outliers, you need to decide what to do with them. If you don't have a big data set,
                taking away even just a few data points could affect your model results. Also, some outliers are
                actually real data and might need to be included in your analysis. However, there are times where the
                outliers need to be removed, or at least be transformed into a different value.
            </p>
            <p>
                If you have a large data set and you identify extreme values using statistical methods, it's probably
                safe to just remove them. If you want or need to keep as much data as possible, you could also transform
                the outliers. A common method is to apply a log transform where you take the logarithm of an outlier to
                reduce its effect on analysis.
            </p>
            <p>
                Outliers aren't the only problem we encounter with real-world data. Often the outliers could be missing
                values that were replaced with a zero or some other value. Removing or changing these values is
                important if we want our analysis and models to accurately represent our data.
            </p>
            <h3>Missing Values</h3>
            <p>
                It would be rare to find a data set that doesn't have missing values. Because there are different ways
                to handle missing values, sometimes we end up with <code>NaN</code> values, but other times there could
                be a <code>0</code> or a <code>999</code> or even some random string. It's also important to figure out
                why there is a missing value. Is it
                because the data wasn't recorded or is it because it didn't make sense to put a value there? For
                example, a survey might ask for the age of the youngest child in the house. If a household doesn't have
                any children, this value might be left empty.
            </p>
            <p>
                Missing values can be handled in different ways depending on why they are missing. The above example
                would be suitable for replacing with <code>NaN</code>. But what if there is a row of data, where the
                column "number of children" is non-zero but the age of the youngest is missing? This data
                could have been
                recorded but wasn't.
                Working with missing data and determining what to do with it will require you to actually look at your
                data and to read any associated documentation. The more important your project, the more closely you
                will want to look at the missing values.
            </p>
            <p>
                In the next section, we'll look at a practice data set and identify some outliers and missing values and
                work through a few examples of how to handle them.
            </p>
            <h3>Follow Along</h3>
            <p>
                We're going to make use of another data set available in the seaborn library. The "planets" data is from
                the <a href="https://exoplanets.nasa.gov/exoplanet-catalog/" target="_blank"
                    rel="noopener noreferrer">NASA exoplanet catalog</a> and contains information
                like the planet's distance from Earth, the planet's mass, and discovery date. This data set contains
                missing values so it will provide us with a good practice set. We can also use this data to check for
                outliers or extreme data points that we might not want to include in our analysis.
            </p>
            <p>
                We'll start by loading the data, looking at the descriptive statistics, and making some plots.
            </p>
            <pre><code># Imports
import seaborn as sns

# Load the data
planets = sns.load_dataset("planets")
display(planets.head())

# Display some of the general information
display(planets.info())
</code></pre>
            <div class="table-responsive">
                <table class="custom-table">
                    <thead>
                        <tr>
                            <th></th>
                            <th>method</th>
                            <th>number</th>
                            <th>orbital_period</th>
                            <th>mass</th>
                            <th>distance</th>
                            <th>year</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>0</td>
                            <td>Radial Velocity</td>
                            <td>1</td>
                            <td>269.300</td>
                            <td>7.10</td>
                            <td>77.40</td>
                            <td>2006</td>
                        </tr>
                        <tr>
                            <td>1</td>
                            <td>Radial Velocity</td>
                            <td>1</td>
                            <td>874.774</td>
                            <td>2.21</td>
                            <td>56.95</td>
                            <td>2008</td>
                        </tr>
                        <tr>
                            <td>2</td>
                            <td>Radial Velocity</td>
                            <td>1</td>
                            <td>763.000</td>
                            <td>2.60</td>
                            <td>19.84</td>
                            <td>2011</td>
                        </tr>
                        <tr>
                            <td>3</td>
                            <td>Radial Velocity</td>
                            <td>1</td>
                            <td>326.030</td>
                            <td>19.40</td>
                            <td>110.62</td>
                            <td>2007</td>
                        </tr>
                        <tr>
                            <td>4</td>
                            <td>Radial Velocity</td>
                            <td>1</td>
                            <td>516.220</td>
                            <td>10.50</td>
                            <td>119.47</td>
                            <td>2009</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1035 entries, 0 to 1034
Data columns (total 6 columns):
 #   Column          Non-Null Count  Dtype  
---  ------          --------------  -----  
 0   method          1035 non-null   object 
 1   number          1035 non-null   int64  
 2   orbital_period  992 non-null    float64
 3   mass            513 non-null    float64
 4   distance        808 non-null    float64
 5   year            1035 non-null   int64  
dtypes: float64(3), int64(2), object(1)
memory usage: 48.6+ KB
</code></pre>
            <p>
                There are a total of 1035 entries in this dataset, and some columns have a non-null count of less than
                1035. We definitely have some missing data. We will try to identity what values are missing and where
                they are located before deciding the next steps.
            </p>
            <pre><code># Count the missing values
print(planets.isnull().sum())
</code></pre>
            <pre><code>method              0
number              0
orbital_period     43
mass              522
distance          227
year                0
dtype: int64
</code></pre>
            <p>
                There are quite a few missing values in the <code>mass</code> column. For a data set like this, it's
                probably because
                there is no data available for that particular observation. In that case, we would likely want to drop
                rows that have missing values.
            </p>
            <pre><code># Drop all rows with missing values
planets_drop = planets.dropna(axis=0, how='any')
display(planets_drop.info())
</code></pre>
            <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 498 entries, 0 to 784
Data columns (total 6 columns):
 #   Column          Non-Null Count  Dtype  
---  ------          --------------  -----  
 0   method          498 non-null    object 
 1   number          498 non-null    int64  
 2   orbital_period  498 non-null    float64
 3   mass            498 non-null    float64
 4   distance        498 non-null    float64
 5   year            498 non-null    int64  
dtypes: float64(3), int64(2), object(1)
memory usage: 27.2+ KB
</code></pre>
            <p>
                We'll make a scatter plot of a few of the values. It would be a somewhat cluttered plot if we used all
                of our variables, so we'll only look at the orbital period, mass, and distance.
            </p>
            <pre><code>import matplotlib.pyplot as plt

# Create axes and figure objects
fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12, 5))

# Plot one set of variables
sns.scatterplot(x="orbital_period", y="distance",       
                data=planets_drop, ax=ax1)

# Plot another variable against the orbital period
sns.scatterplot(x="orbital_period", y="mass",           
                data=planets_drop, ax=ax2)

fig.show()
</code></pre>
            <p><img src="https://raw.githubusercontent.com/bloominstituteoftechnology/data-science-canvas-images/main/unit_2/sprint_2/mod1_obj1_planets_scatter.png"
                    alt="Two scatter plot graphs. The first shows the relationship between distance and orbital period. The second shows the relationship between mass and orbital period."
                    width="1068" height="445" loading="lazy"></p>
            <p>
                We have a few data points that might be outliers, but they could also be planets that just have very
                long orbital periods. We'll make some box plots to see how these outlying values compare to the median
                of the distributions.
            </p>
            <pre><code># Box plot

# Create axes and figure objects
fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12, 5));

sns.boxplot(y=planets_drop["orbital_period"], ax=ax1)
sns.boxplot(y=planets_drop["mass"], ax=ax2)
sns.boxplot(y=planets_drop["distance"], ax=ax3)

fig.show()
</code></pre>
            <p><img src="https://raw.githubusercontent.com/bloominstituteoftechnology/data-science-canvas-images/main/unit_2/sprint_2/mod1_obj1_planetsbox.png"
                    alt="Three box plot graphs showing the following data points from left to right: orbital period, mass, and distance. "
                    width="1068" height="445" loading="lazy"></p>
            <h3>Challenge</h3>
            <p>
                For this challenge, you can choose to explore a dataset you are already somewhat familiar with. Check
                for missing values, think about what you should do with them, and then make some plots to help identify
                outliers.
            </p>
            <h3>Additional Resources</h3>
            <ul>
                <li>
                    <a href="https://www.kaggle.com/rtatman/data-cleaning-challenge-handling-missing-values"
                        target="_blank" rel="noopener noreferrer">Kaggle: Decision Trees Tutorial</a>
                </li>
                <li>
                    <a href="https://exoplanets.nasa.gov/exoplanet-catalog/" target="_blank"
                        rel="noopener noreferrer">Kaggle: Handling Missing Values</a>
                </li>
            </ul>
        </section>

        <section class="content-box">
            <h2>Objective 02 - use scikit-learn for decision trees</h2>
            <h3>Overview</h3>
            <p>
                Decision trees are a type of supervised learning that can be used for both classification and regression
                problems. Their advantages are that they are easy to understand and visualize, and can handle both
                numeric and categorical data.
            </p>
            <h3>Decision Tree Basics</h3>
            <p>
                A decision tree algorithm is pretty straightforward. We have a data set that we split or fork by asking
                a series of questions. The data is evaluated at each question, or node, and then split according to the
                answer to that question. Each split is called a branch, and each branch ends in a node.
            </p>
            <p>
                So how do we decide what to split the node on? Since decision trees can be used for both regression and
                classification tasks, we use two different methods to split on a node. For a regression task with
                continuous variables, we minimize the variance of the values. For a classification task, the Gini
                impurity is used to measure the "purity" of the split. If all the values belong to one class the node
                has the maximum purity.
            </p>
            <p>
                A decision tree can have as many layers as needed. Usually a node has two branches, but it can have
                more. We stop branching when there is no reduction in either the variance or the Gini impurity value.
            </p>
            <h3>Follow Along</h3>
            <p>
                We'll implement a decision tree in scikit-learn with the penguins data from the previous objective. We
                want to classify each penguin as male or female based on the physical characteristics and the species.
            </p>
            <pre><code># Imports!
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder

# Use the decision tree classifier 
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

# Set-up the one-hot encoder method
categorical_features = ['species']
categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder())])

# Set up our preprocessor/column transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', categorical_transformer, categorical_features)])

# Add the classifier to the preprocessing pipeline
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                      ('classifier', DecisionTreeClassifier())])</code></pre>
            <pre><code># Load in the data!

import pandas as pd
import seaborn as sns

penguins = sns.load_dataset("penguins")
penguins.dropna(inplace=True)

# Select features
features = ['species', 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
X = penguins[features]

# Encode the 'sex' column
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
penguins['sex_encode'] = le.fit_transform(penguins['sex'])

# Set target array
y = penguins['sex_encode']

# Apply the pipeline

# Separate into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

# Fit the model with our logistic regression classifier
pipeline.fit(X_train, y_train)
print("model score: %.3f" % pipeline.score(X_test, y_test))
</code></pre>
            <pre><code>model score: 0.417
</code></pre>
            <p>
                It looks like we have a model that performs slightly better than the logistic regression model from
                earlier!
            </p>
            <h3>Challenge</h3>
            <p>
                The decision tree classifier has several parameters that can be adjusted. Another extension on this
                objective would be to change the features that the model is trained on. Try removing the encoded
                categorical columns and train only using the numeric columns.
            </p>
            <h3>Additional Resources</h3>
            <ul>
                <li>
                    <a href="https://scikit-learn.org/stable/modules/tree.html#tree" target="_blank"
                        rel="noopener noreferrer">Scikit-learn User Guide: Decision Tree</a>
                </li>
                <li>
                    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"
                        target="_blank" rel="noopener noreferrer">Scikit-learn: Decision Tree Classifier</a>
                </li>
            </ul>
        </section>

        <section class="content-box">
            <h2>Objective 03 - get and interpret feature importances of a tree-based model</h2>
            <h3>Overview</h3>
            <p>
                When we evaluate a linear model we look at the coefficients for each of the parameters and analyze the
                importance of each parameter to the model. Decision tree models don't have coefficients. Instead we look
                at feature importance when interpreting a model.
            </p>
            <p>
                The overall importance of a feature in a decision tree has a relatively simple interpretation. For the
                tree we go through all of the splits for which the feature was used and then determine how much it has
                reduced the variance or Gini index compared to the parent node. If the feature has a large share of the
                reduction, then it has a greater importance for the model. Another way to look at feature importance is
                as a measure of how early and often a feature is used for the tree's "branching" decisions.
            </p>
            <p>
                Like most predictive modeling tools and techniques, feature importances are useful but have trade-offs.
                For example, they can make assumptions and be misinterpreted. We'll continue to discuss this throughout
                this module and subsequent sprints.
            </p>
            <h3>Follow Along</h3>
            <p>
                In this section, we'll implement a decision tree model on a new data set about wine quality. This data
                is available with the <a
                    href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html"
                    target="_blank" rel="noopener noreferrer">scikit-learn dataset library</a>.
            </p>
            <p>
                The wine dataset is a classic and very easy multi-class classification dataset.
            </p>
            <ul>
                <li>Three classes with samples per class of: <code>[59,71,48]</code></li>
                <li>Samples total - 178</li>
                <li>Dimensionality - 13</li>
                <li>Features - real, positive</li>
            </ul>
            <p>
                The goal is to classify wine into one of three classes using the characteristic features such as alcohol
                content, flavor, hue, etc.
            </p>
            <pre><code># Import libraries and data sets
from sklearn.datasets import load_wine
import pandas as pd

# Load the data and convert to a DataFrame
data = load_wine()
df_wine = pd.DataFrame(data.data, columns=data.feature_names)
df_wine['target'] = pd.Series(data.target)

display(df_wine.shape)
df_wine.head()</code></pre>
            <pre><code>(178, 14)</code></pre>
            <div class="table-responsive">
                <table class="custom-table">
                    <thead>
                        <tr>
                            <th></th>
                            <th>alcohol</th>
                            <th>malic_acid</th>
                            <th>ash</th>
                            <th>alcalinity_of_ash</th>
                            <th>magnesium</th>
                            <th>total_phenols</th>
                            <th>flavanoids</th>
                            <th>nonflavanoid_phenols</th>
                            <th>proanthocyanins</th>
                            <th>color_intensity</th>
                            <th>hue</th>
                            <th>od280/od315_of_diluted_wines</th>
                            <th>proline</th>
                            <th>target</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>0</td>
                            <td>14.23</td>
                            <td>1.71</td>
                            <td>2.43</td>
                            <td>15.6</td>
                            <td>127.0</td>
                            <td>2.80</td>
                            <td>3.06</td>
                            <td>0.28</td>
                            <td>2.29</td>
                            <td>5.64</td>
                            <td>1.04</td>
                            <td>3.92</td>
                            <td>1065.0</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>1</td>
                            <td>13.20</td>
                            <td>1.78</td>
                            <td>2.14</td>
                            <td>11.2</td>
                            <td>100.0</td>
                            <td>2.65</td>
                            <td>2.76</td>
                            <td>0.26</td>
                            <td>1.28</td>
                            <td>4.38</td>
                            <td>1.05</td>
                            <td>3.40</td>
                            <td>1050.0</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>2</td>
                            <td>13.16</td>
                            <td>2.36</td>
                            <td>2.67</td>
                            <td>18.6</td>
                            <td>101.0</td>
                            <td>2.80</td>
                            <td>3.24</td>
                            <td>0.30</td>
                            <td>2.81</td>
                            <td>5.68</td>
                            <td>1.03</td>
                            <td>3.17</td>
                            <td>1185.0</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>3</td>
                            <td>14.37</td>
                            <td>1.95</td>
                            <td>2.50</td>
                            <td>16.8</td>
                            <td>113.0</td>
                            <td>3.85</td>
                            <td>3.49</td>
                            <td>0.24</td>
                            <td>2.18</td>
                            <td>7.80</td>
                            <td>0.86</td>
                            <td>3.45</td>
                            <td>1480.0</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>4</td>
                            <td>13.24</td>
                            <td>2.59</td>
                            <td>2.87</td>
                            <td>21.0</td>
                            <td>118.0</td>
                            <td>2.80</td>
                            <td>2.69</td>
                            <td>0.39</td>
                            <td>1.82</td>
                            <td>4.32</td>
                            <td>1.04</td>
                            <td>2.93</td>
                            <td>735.0</td>
                            <td>0</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <p>
                Here we have 13 features and one target column. The features are numeric so we won't need to worry about
                categorical encoding for this example. We first need to create our feature matrix and target array.
            </p>
            <pre><code># Separate into features and target
X = df_wine.drop('target', axis=1)
y = df_wine['target']

# Import train_test_split function
from sklearn.model_selection import train_test_split

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)</code></pre>
            <pre><code># Use the decision tree classifier 
from sklearn.tree import DecisionTreeClassifier

# Instantiate the classifier
classifier=DecisionTreeClassifier()

# Train the model using the training sets
classifier.fit(X_train,y_train)

# Find the model score
print("Decision tree model score: %.3f" % classifier.score(X_test, y_test))</code></pre>
            <pre><code>Decision tree model score: 0.933</code></pre>
            <p>
                We fit a decision tree model! The results look good. The model seems to be able to predict the class of
                wine quite well given the 13 characteristics. Now, let's look at feature importance. We do this by
                plotting each feature's contribution to the model on a bar chart. The total contribution of all the
                features is normalized to 100 (or sometimes 1), so each feature is some percentage of that.
            </p>
            <pre><code># Plot the feature importances
import matplotlib.pyplot as plt

importances = pd.Series(classifier.feature_importances_, X.columns)

# Plot top n feature importances
n = 13
plt.figure(figsize=(10,n/2))
plt.title(f'Top {n} features')
importances.sort_values()[-n:].plot.barh()

plt.show()</code></pre>
            <pre><code>&lt;Figure size 720x468 with 0 Axes&gt;</code></pre>

            <p><img src="https://raw.githubusercontent.com/bloominstituteoftechnology/data-science-canvas-images/main/unit_2/sprint_2/mod1_obj4_tree_wine.png"
                    alt="mod1_obj4_tree_wine.png" loading="lazy"></p>
            <p>
            <p>
                For our model, it looks like the top three features contribute the most to the model, by a significant
                fraction.
            </p>
            <h3>Challenge</h3>
            <p>
                For the above model, we used the default parameters. Using the scikit-learn documentation, explore some
                of the other parameters. Using the above code, run the model again, but with different parameters. A few
                to try would be <code>criterion</code> (how to split) and the <code>max_depth</code> (how many nodes).
            </p>
            <h3>Additional Resources</h3>
            <ul>
                <li>
                    <a href="https://scikit-learn.org/stable/modules/tree.html#tree" target="_blank"
                        rel="noopener noreferrer">Scikit-learn User Guide: Decision Tree</a>
                </li>
                <li>
                    <a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"
                        target="_blank" rel="noopener noreferrer">Scikit-learn: Decision Tree Classifier</a>
                </li>
            </ul>
        </section>

        <section class="content-box">
            <h2>Guided Project</h2>
            <p>
                Open <strong>JDS_SHR_221_guided_project_notes.ipynb</strong> in the
                GitHub repository below to follow along with the guided project:
            </p>
            <div class="resource-links">
                <a href="https://github.com/bloominstituteoftechnology/DS-Unit-2-Kaggle-Challenge/tree/main/module1-decision-trees"
                    class="resource-link" target="_blank" rel="noopener noreferrer">GitHub: Decision Trees</a>
                <a href="https://docs.google.com/presentation/d/16VzItcnCQA7in9wyMH9YKHoVa6bigs0wc90LZlwrdjk/present"
                    class="resource-link" target="_blank" rel="noopener noreferrer">Slides</a>
            </div>

            <h2>Guided Project Video</h2>
            <div class="video-container">
                <iframe class="wistia_embed" title="Sprint 6 Decision Trees Video"
                    src="https://fast.wistia.net/embed/iframe/40x4jmc9gp" width="640" height="360" allow="fullscreen"
                    loading="lazy"></iframe>
            </div>
        </section>

        <section class="content-box">
            <h2>Module Assignment</h2>
            <p>
                Complete the Module 1 assignment to practice decision tree techniques
                you've learned.
            </p>
            <div class="resource-links">
                <a href="https://github.com/bloominstituteoftechnology/DS-Unit-2-Kaggle-Challenge/blob/main/module1-decision-trees/LS_DS_221_assignments.ipynb"
                    class="resource-link" target="_blank" rel="noopener noreferrer">Module 1 Assignment</a>
            </div>

            <p>
                It's Kaggle competition time! In this assignment, you'll apply what
                you've learned about decision trees to a real-world dataset.
            </p>

            <h3>Getting Started with Kaggle</h3>
            <p>
                If this is your first time using Kaggle, here's how to get started:
            </p>
            <ul>
                <li>
                    <strong>Create an Account:</strong> Visit
                    <a href="https://www.kaggle.com/account/login" target="_blank"
                        rel="noopener noreferrer">Kaggle.com</a>
                    and register with your email
                </li>
                <li>
                    <strong>Join the Competition:</strong> Navigate to the competition
                    page and click "Join Competition"
                </li>
                <li>
                    <strong>Download Data:</strong> Go to the "Data" tab and download
                    the dataset files
                </li>
            </ul>
            <p>Watch this walkthrough video for detailed instructions:</p>
            <div class="video-container">
                <iframe class="wistia_embed" title="DS - Unit 2 - Kaggle Challenge Set Up Video"
                    src="https://fast.wistia.net/embed/iframe/bt12x1csuf" width="640" height="360" allow="fullscreen"
                    loading="lazy"></iframe>
            </div>

            <h3>Additional Kaggle Resources</h3>
            <div class="resource-links">
                <a href="https://docs.google.com/document/d/1rUUEWrrD1iwxKNshDx1uPBwdOp-aSgxpg-qY2cIyDO0/edit"
                    class="resource-link" target="_blank" rel="noopener noreferrer">Kaggle Challenge Setup Resource</a>
                <a href="https://www.kaggle.com/getting-started/44915" class="resource-link" target="_blank"
                    rel="noopener noreferrer">Getting Started with Kaggle</a>
                <a href="https://www.kaggle.com/docs/competitions" class="resource-link" target="_blank"
                    rel="noopener noreferrer">Kaggle Competitions Documentation</a>
            </div>

            <h2>Assignment Solution Video</h2>
            <div class="video-container">
                <iframe class="wistia_embed" title="Decision Trees - Project Solution Video"
                    src="https://fast.wistia.net/embed/iframe/n9luvwg7qj" width="640" height="360" allow="fullscreen"
                    loading="lazy"></iframe>
            </div>
        </section>

        <section class="content-box">
            <h2>Resources</h2>

            <h3>Documentation</h3>
            <ul>
                <li>
                    <a href="https://scikit-learn.org/stable/modules/tree.html" target="_blank"
                        rel="noopener noreferrer">Scikit-learn: Decision Trees</a>
                </li>
                <li>
                    <a href="https://scikit-learn.org/stable/modules/impute.html" target="_blank"
                        rel="noopener noreferrer">Scikit-learn: Imputation of missing values</a>
                </li>
                <li>
                    <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html" target="_blank"
                        rel="noopener noreferrer">Pandas: Working with missing data</a>
                </li>
            </ul>

            <h3>Tutorials</h3>
            <ul>
                <li>
                    <a href="https://www.kaggle.com/code/prashant111/decision-tree-classifier-tutorial" target="_blank"
                        rel="noopener noreferrer">Kaggle: Decision Trees Tutorial</a>
                </li>
                <li>
                    <a href="https://www.kaggle.com/alexisbcook/missing-values" target="_blank"
                        rel="noopener noreferrer">Kaggle: Handling Missing Values</a>
                </li>
            </ul>
        </section>
    </main>
</body>

</html>