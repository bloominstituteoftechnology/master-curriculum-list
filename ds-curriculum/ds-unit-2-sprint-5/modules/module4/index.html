<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>DS5 Module 4 - Logistic Regression</title>
    <link rel="stylesheet" href="../../../css/style.css" />
  </head>

  <body>
    <header>
      <nav>
        <div class="logo">Data Science Unit 2</div>
        <ul>
          <li><a href="../../index.html">Home</a></li>
          <li class="dropdown">
            <a href="#" class="active">Modules</a>
            <div class="dropdown-content">
              <a href="../module1/index.html">Module 1: Linear Regression 1</a>
              <a href="../module2/index.html">Module 2: Linear Regression 2</a>
              <a href="../module3/index.html">Module 3: Ridge Regression</a>
              <a href="../module4/index.html" class="active"
                >Module 4: Logistic Regression</a
              >
            </div>
          </li>
          <li><a href="../../code-alongs/index.html">Code-Alongs</a></li>
          <li>
            <a href="../../sprint-challenge/index.html">Sprint Challenge</a>
          </li>
        </ul>
      </nav>
    </header>

    <main class="container">
      <h1>Module 4: Logistic Regression</h1>

      <section class="content-box">
        <h2>Module Overview</h2>
        <p>
          In this module, you will transition from regression to classification
          with logistic regression. You'll implement train-validate-test splits,
          understand classification baselines, and learn about scikit-learn
          pipelines. These skills will enable you to build and evaluate models
          for binary classification problems.
        </p>
      </section>

      <section class="content-box">
        <h2>Learning Objectives</h2>
        <ul>
          <li>Implement a train-validate-test split</li>
          <li>Begin with baselines for classification</li>
          <li>
            Express and explain the intuition and interpretation of logistic
            regression
          </li>
          <li>
            Use scikit-learn to fit and interpret logistic regression models
          </li>
          <li>Use scikit-learn pipelines</li>
        </ul>
      </section>

      <section class="content-box">
        <h2>Objective 01 - Implement a train-validate-test split</h2>
        <h3>Overview</h3>
        <p>
          In the previous module we used a train-test split, where we hold back
          a subset of the data to use for testing the model. When we train a
          model we also need to evaluate the model. Recall that if we evaluate
          on the training data we're not getting an accurate estimate of the
          true performance of the model. For this reason, we need to use test
          data that the model has not yet seen.
        </p>
        <p>
          Sometimes it's useful to be able to have an intermediate step where
          the model can be evaluated without using the set-aside test set. This
          is where a validation set is useful. Consider the situation where we
          take a subset of our data and set it aside as the test set - we won't
          touch this data until we're ready to evaluate a final model.
        </p>
        <p>
          With the remaining data, we can divide it into training and validation
          sets. We then train the model on the training data and evaluate it on
          the validation data. Another advantage of using a validation set is
          that it can be used to tune the model or adjust the hyperparameters.
          Iterations of tuning and model fitting are used to find the final
          model, which is then evaluated using the test set.
        </p>
        <h3>Train-validate-test</h3>
        <p>Some general definitions are:</p>
        <ul>
          <li>
            <strong>training dataset</strong>: the sample of data used to fit
            the model
          </li>
          <li>
            <strong>validation dataset</strong>: the sample of data used to
            evaluate the model and possibly to adjust the hyperparameters
          </li>
          <li>
            <strong>testing dataset</strong>: the sample of data used for final
            model testing; not to be used for anything other than testing so
            that the result is unbiased
          </li>
        </ul>
        <p>
          One last point to make is that sometimes you won't even have access
          the test set! If you are participating in a Kaggle competition, for
          example, you cannot view the actual target values for the test data,
          and can only generate the predictions for your submission. The number
          of test prediction submissions might be limited, or you might not want
          to make numerous test submissions just to evaluate or tune your model.
        </p>
        <p>
          In this next section, we'll create our own train-validation-test data
          sets. We'll follow the guideline of using 60% for training, 20% for
          validation, and 20% for testing.
        </p>
        <h3>Follow Along</h3>
        <p>
          We haven't yet worked with the Iris dataset in this module, so we'll
          start there. In the following example, we load the data and then
          separate out the feature <code>petal_width</code> and the target
          <code>petal_length</code>. Having plotted this data earlier, we know
          there is a linear relationship between the petal width and length: the
          wider the petal, the greater the length. We'll use a linear model to
          predict our target.
        </p>
        <pre><code># Import numpy and seaborn
import numpy as np
import seaborn as sns

iris = sns.load_dataset("iris")
display(iris.head())

x = iris['petal_width'] 
X = np.array(x)[:, np.newaxis]
y = iris['petal_length']
</code></pre>
        <div class="table-responsive">
          <table class="custom-table">
            <thead>
              <tr>
                <th></th>
                <th>sepal_length</th>
                <th>sepal_width</th>
                <th>petal_length</th>
                <th>petal_width</th>
                <th>species</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>0</td>
                <td>5.1</td>
                <td>3.5</td>
                <td>1.4</td>
                <td>0.2</td>
                <td>setosa</td>
              </tr>
              <tr>
                <td>1</td>
                <td>4.9</td>
                <td>3.0</td>
                <td>1.4</td>
                <td>0.2</td>
                <td>setosa</td>
              </tr>
              <tr>
                <td>2</td>
                <td>4.7</td>
                <td>3.2</td>
                <td>1.3</td>
                <td>0.2</td>
                <td>setosa</td>
              </tr>
              <tr>
                <td>3</td>
                <td>4.6</td>
                <td>3.1</td>
                <td>1.5</td>
                <td>0.2</td>
                <td>setosa</td>
              </tr>
              <tr>
                <td>4</td>
                <td>5.0</td>
                <td>3.6</td>
                <td>1.4</td>
                <td>0.2</td>
                <td>setosa</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>
          First, we'll hold back a subset of the data just for the test data.
          We'll do this with the scikit-learn utility. We'll call it something
          different from "train" so that we don't confuse it with the actual
          training data later.
        </p>
        <pre><code># Import the train_test_split utility
from sklearn.model_selection import train_test_split

# Create the "remaining" and test datasets
X_remain, X_test, y_remain, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)
</code></pre>
        <p>
          Then we'll create a training set and validation set from the remaining
          data. We could have done this in one step but we're breaking it down
          here so it's easier to see that we removed a test subset and will not
          accidentally use it for evaluation until we're ready to test.
        </p>
        <pre><code># Create the train and validation datasets

X_train, X_val, y_train, y_val = train_test_split(
    X_remain, y_remain, test_size=0.25, random_state=42)

# Print out sizes of train, validate, test datasets

print('Training data set samples:', len(X_train))
print('Validation data set samples:', len(X_val))
print('Test data set samples:', len(X_test))
</code></pre>
        <pre><code>Training data set samples: 90
Validation data set samples: 30
Test data set samples: 30
</code></pre>
        <p>Now we can fit our model and evaluate it on our validation set.</p>
        <pre><code># Import the predictor and instantiate the class
from sklearn.linear_model import LinearRegression

# Instantiate the model
model = LinearRegression()

# Fit the model
model.fit(X_train, y_train)

# Use the VALIDATION set for prediction
y_predict = model.predict(X_val)

# Calculate the accuracy score
from sklearn.metrics import r2_score
r2_score(y_val, y_predict)
</code></pre>
        <pre><code>0.9589442606386026
</code></pre>
        <p>
          Well, that's a pretty good model score (R-squared), which we expect
          because we know the Iris dataset has a strong linear trend between the
          petal width and petal length. Now would be the time to change any of
          the model hyperparameters and evaluate on the validation set again.
          We'll continue with the default model parameters for now.
          Hyperparameter tuning is something that will be introduced in the
          later Sprints.
        </p>
        <p>Now, let's use the test set we held back above.</p>
        <pre><code># Use the TEST set for prediction
y_predict_test = model.predict(X_test)

# Calculate the accuracy score

r2_score(y_test, y_predict_test)
</code></pre>
        <pre><code>0.9287783612248339
</code></pre>
        <p>
          The R-squared score is a little lower than it was for the validate
          set. If we were to run the model and test again with a different
          random seed, the scores would be different and the test score might be
          higher.
        </p>
        <h3>Challenge</h3>
        <p>
          Using the same data set as in the example, try changing the
          <code>random_state</code> parameter to see how the validate and test
          model scores change.
        </p>
        <h3>Additional Resources</h3>
        <ul>
          <li>
            <a
              href="https://machinelearningmastery.com/difference-test-validation-datasets/"
              target="_blank"
              rel="noopener noreferrer"
            >
              What is the Difference Between Test and Validation Datasets?<!--Links to an external site.-->
            </a>
          </li>
        </ul>
      </section>

      <section class="content-box">
        <h2>Objective 02 - Begin with baselines for classification</h2>
        <h3>Overview</h3>
        <p>
          When we fit a model to our data and look at the model score, we need
          something to compare that score with. As we covered in module 1, a
          baseline is a simple estimate or prediction for a model. A baseline
          can be determined from descriptive statistics such as the mean value
          of a variable, or even a simple linear regression for two variables.
          We started by finding a baseline for a linear regression problem where
          we were predicting continuous variables. From our penguin data set we
          used a baseline estimate of the ratio of flipper length to body mass.
        </p>
        <p>
          In this module, we are going to focus on classification problems. A
          classification model predicts which class a set of observations
          belongs to. Classification problems deal with discrete or
          non-continuous variables. As an example of a classification problem,
          consider the Old Faithful geyser data set where we have some
          information about how long each eruption lasts and the length of time
          between eruptions. There are two features (<code>duration</code> and
          <code>waiting</code>) and one target containing two classes
          (<code>kind</code>). We want to use the <code>duration</code> and
          <code>waiting</code> features to predict which class the geyser
          eruption belongs to (<code>long</code> or <code>short</code>).
        </p>
        <h3>Classification Baseline</h3>
        <p>
          Before we set up our model we need to start with a baseline. For a
          classification problem a common starting place is to find the most
          common class and use that as a baseline. We'll start by considering a
          binary classification problem, where there are only two classes. In
          our geyser data set there are two kinds of geyser eruptions, short and
          long, so this data set is suitable for a binary classification
          problem.
        </p>
        <p>
          Why do we use the most common class as a starting baseline? If we
          think about how we make a prediction, we would most likely be correct
          if our guess is the most common class. Let's explore the data set to
          find the most common class and then calculate the accuracy of this
          baseline.
        </p>
        <h3>Follow Along</h3>
        <p>
          The eruption data set is available in the
          <code>seaborn</code> plotting library data sets, so it's easy to load
          and use. We'll start the usual way by loading the data and viewing it.
        </p>
        <pre><code># Import numpy and seaborn
import numpy as np
import seaborn as sns

geyser = sns.load_dataset("geyser")
display(geyser.head())

display(geyser.describe())
</code></pre>
        <div class="table-responsive">
          <table class="custom-table">
            <thead>
              <tr>
                <th></th>
                <th>duration</th>
                <th>waiting</th>
                <th>kind</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>0</td>
                <td>3.600</td>
                <td>79</td>
                <td>long</td>
              </tr>
              <tr>
                <td>1</td>
                <td>1.800</td>
                <td>54</td>
                <td>short</td>
              </tr>
              <tr>
                <td>2</td>
                <td>3.333</td>
                <td>74</td>
                <td>long</td>
              </tr>
              <tr>
                <td>3</td>
                <td>2.283</td>
                <td>62</td>
                <td>short</td>
              </tr>
              <tr>
                <td>4</td>
                <td>4.533</td>
                <td>85</td>
                <td>long</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive">
          <table class="custom-table">
            <thead>
              <tr>
                <th></th>
                <th>duration</th>
                <th>waiting</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>count</td>
                <td>272.000000</td>
                <td>272.000000</td>
              </tr>
              <tr>
                <td>mean</td>
                <td>3.487783</td>
                <td>70.897059</td>
              </tr>
              <tr>
                <td>std</td>
                <td>1.141371</td>
                <td>13.594974</td>
              </tr>
              <tr>
                <td>min</td>
                <td>1.600000</td>
                <td>43.000000</td>
              </tr>
              <tr>
                <td>25%</td>
                <td>2.162750</td>
                <td>58.000000</td>
              </tr>
              <tr>
                <td>50%</td>
                <td>4.000000</td>
                <td>76.000000</td>
              </tr>
              <tr>
                <td>75%</td>
                <td>4.454250</td>
                <td>82.000000</td>
              </tr>
              <tr>
                <td>max</td>
                <td>5.100000</td>
                <td>96.000000</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>
          The next step is to see how many observations are in each class. We
          can use the <code>value_counts()</code> method on the
          <code>kind</code> column.
        </p>
        <pre><code># Find the number of counts for each type of eruption
geyser['kind'].value_counts()
</code></pre>
        <pre><code>long     172
short    100
Name: kind, dtype: int64
</code></pre>
        <p>
          Here, we have more observations for the <code>long</code> class. If we
          were given a set of values for the duration and waiting interval and
          predicted <code>long</code> for the class, we would be correct 63% of
          the time (number of <code>long</code> values divided by the total
          number of observations: <code>172/272 = 0.63</code>). This baseline is
          the number we would like to beat when we actually train and fit a
          model to our data set.
        </p>
        <h3>Challenge</h3>
        <p>
          Using the penguin data set and the iris data set, find the most common
          classes for the target variables ("sex" for the penguin data and
          "species" for the iris data). For each of these data sets, what is the
          baseline?
        </p>
        <h3>Additional Resources</h3>
        <ul>
          <li>
            <a
              href="http://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat"
              target="_blank"
              rel="noopener noreferrer"
            >
              Old Faithful Geyser Data
            </a>
          </li>
        </ul>
      </section>

      <section class="content-box">
        <h2>
          Objective 03 - Express and explain the intuition and interpretation of
          logistic regression
        </h2>
        <h3>Overview</h3>
        <p>
          So far in this sprint, we have been fitting linear regression models
          to continuous data and predicting a numeric value. In this next part
          of the module, we're going to be implementing a classification model
          where we predict the class a given observation belongs to. One of the
          most basic classification techniques is called a logistic regression.
          Don't confuse linear regression with logistic regression, the former
          is a regression model and the latter used for classification.
        </p>
        <h3>Logistic Regression</h3>
        <p>
          A logistic regression classifier is based on the sigmoid function
          which is an s-shaped curve. The equation for the sigmoid is given by:
        </p>
        <p>
          <img
            class="eq-img"
            src="https://i.upmath.me/svg/%5Csigma(x)%20%3D%20%5Cfrac%7B1%7D%7B1%2Bexp(-x)%7D"
            alt="\sigma(x) = \frac{1}{1+exp(-x)}"
            align="center"
            loading="lazy"
          />
        </p>
        <p>
          We'll plot this function and then discuss why it's shape makes it
          suitable for a binary classification problem.
        </p>
        <pre><code># The logistic sigmoid function (implemented to accept numpy arrays)
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.e**(-x))</code></pre>
        <pre><code># Plot the function

x_plot = np.linspace(-10, 10, 100)
sig_y = sigmoid(x_plot)

# Imports for plotting
import matplotlib.pyplot as plt

# Plot the function generated above
plt.plot(x_plot, sig_y)
plt.xlabel('x'); plt.ylabel('$\sigma(x)$')
plt.title('Sigmoid function')

plt.show()
</code></pre>
        <p>
          <img
            class="eq-img"
            src="https://raw.githubusercontent.com/bloominstituteoftechnology/data-science-canvas-images/main/unit_2/sprint_1/mod4_obj3_sigmoid.png"
            alt="mod4_obj3_sigmoid.png"
            loading="lazy"
          />
        </p>
        <p>
          Over most of the range of the sigmoid function, the value is either 0
          or 1, which is why this function is particularly suitable for a binary
          classifier. When we are fitting a model, we would like to find the
          coefficients that best fit the data. Including these coefficients
          results in an equation of this form:
        </p>
        <p>
          <img
            class="eq-img"
            src="https://i.upmath.me/svg/P(y_i%20%3D%201)%20%3D%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B-(%5Cbeta_0%2B%5Cbeta_1x)%7D%7D"
            alt="P(y_i = 1) = \frac{1}{1+e^{-(\beta_0+\beta_1x)}}"
            align="center"
            loading="lazy"
          />
        </p>
        <p>
          where
          <img
            class="eq-small"
            src="https://i.upmath.me/svg/P(y_i%3D1)"
            alt="P(y_i=1)"
            loading="lazy"
          />
          is the probability of observation
          <img
            class="eq-small"
            src="https://i.upmath.me/svg/i"
            alt="i"
            loading="lazy"
          />
          being in class 1. The coefficients
          <img
            class="eq-small"
            src="https://i.upmath.me/svg/%5Cbeta_0"
            alt="\beta_0"
            loading="lazy"
          />
          and
          <img
            class="eq-small"
            src="https://i.upmath.me/svg/%5Cbeta_1"
            alt="\beta_1"
            loading="lazy"
          />
          determine the shape of the function and are what we are trying to fit
          when we model our data. When we know the coefficients, we can make a
          prediction of the class for an observation
          <img
            class="eq-small"
            src="https://i.upmath.me/svg/x"
            alt="x"
            loading="lazy"
          />.
        </p>
        <h3>Follow Along</h3>
        <pre><code># Import seaborn and load the data
import seaborn as sns

geyser = sns.load_dataset("geyser")

# Choose one feature - we'll use the duration
x = geyser['duration']

# Import the label encoder and encode the 'kind' column
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

# Create a new column with 0=long and 1=short class labels
geyser['kind_binary'] = le.fit_transform(geyser['kind'])
display(geyser.head())

# Assign the target variable to y
y = geyser['kind_binary']
</code></pre>
        <table class="custom-table">
          <thead>
            <tr>
              <th></th>
              <th>duration</th>
              <th>waiting</th>
              <th>kind</th>
              <th>kind_binary</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>0</td>
              <td>3.600</td>
              <td>79</td>
              <td>long</td>
              <td>0</td>
            </tr>
            <tr>
              <td>1</td>
              <td>1.800</td>
              <td>54</td>
              <td>short</td>
              <td>1</td>
            </tr>
            <tr>
              <td>2</td>
              <td>3.333</td>
              <td>74</td>
              <td>long</td>
              <td>0</td>
            </tr>
            <tr>
              <td>3</td>
              <td>2.283</td>
              <td>62</td>
              <td>short</td>
              <td>1</td>
            </tr>
            <tr>
              <td>4</td>
              <td>4.533</td>
              <td>85</td>
              <td>long</td>
              <td>0</td>
            </tr>
          </tbody>
        </table>
        <p>
          We now have a DataFrame with a column encoded with two classes: long=0
          and short=1. Let's plot the <code>duration</code> column against the
          binary classes we just created.
        </p>
        <pre><code># Plot the data for 'duration'
plt.scatter(x, y)
plt.yticks([0, 1])
plt.xlabel('x (geyser duration - minutes)'); plt.ylabel('kind of eruption')
plt.title('Geyser duration')

plt.show()
</code></pre>
        <p>
          <img
            src="https://raw.githubusercontent.com/bloominstituteoftechnology/data-science-canvas-images/main/unit_2/sprint_1/mod4_obj3_geyser.png"
            alt="mod4_obj3_geyser.png"
            loading="lazy"
          />
        </p>
        <p>
          Now, let's use the sigmoid function with coefficients from a model
          that fits the above data. We'll get to this step in the next objective
          so for now let's just focus on what the coefficient means and how to
          interpret the result. We'll assign the coefficients to variables and
          then plot the new function on our data above.
        </p>
        <pre><code># Assign coefficient from previously fit model
beta_0 = 11.32
beta_1 = -3.65

# Define the sigmoid with the coefficients
def sigmoid_beta(x, beta_0, beta_1):
    exp = beta_0 + beta_1*x
    return 1 / (1 + np.e**(-exp))

x_model_plot = np.linspace(1, 6, 100)
y_model = sigmoid_beta(x_model_plot, beta_0, beta_1)

# Plot the function generated above
plt.scatter(x, y)
plt.plot(x_model_plot, y_model, color='green', label='model')
plt.xlabel('x (geyser duration - minutes)'); plt.ylabel('P( y=1 )')
plt.legend()
plt.title('Geyser eruption with model')

plt.show()
</code></pre>
        <p>
          <img
            src="https://raw.githubusercontent.com/bloominstituteoftechnology/data-science-canvas-images/main/unit_2/sprint_1/mod4_obj3_geyser_model.png"
            alt="mod4_obj3_geyser_model.png"
            loading="lazy"
          />
        </p>
        <p>
          Now let's use our function with the model parameters to make and
          interpret a prediction. We'll pretend that we have visited the geyser
          site and viewed an eruption, which we timed to be 3.25 minutes. Which
          class would this eruption belong to and with what probability?
        </p>
        <p>
          We know from the equation above that the probability is for the
          observation to belong to class=1 (short eruption). Plugging in the
          values for x (3.25 minutes) along with the coefficients gives us the
          following equation:
        </p>
        <p>
          <img
            class="eq-img"
            src="https://i.upmath.me/svg/%20P(y_i%20%3D%201)%20%3D%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B-(%5Cbeta_0%2B%5Cbeta_1x)%7D%7D%20"
            alt=" P(y_i = 1) = \frac{1}{1+e^{-(\beta_0+\beta_1x)}} "
            align="center"
            loading="lazy"
          />
        </p>
        <p>
          <img
            class="eq-img"
            src="https://i.upmath.me/svg/%20P(y_i%20%3D%201%20%5Ctext%7B%20when%20%7Dx%3D3.25)%20%3D%20%5Cfrac%7B1%7D%7B1%2Be%5E%7B-(11.32-3.65%5Ctimes3.25)%7D%7D"
            alt=" P(y_i = 1 \text{ when }x=3.25) = \frac{1}{1+e^{-(11.32-3.65\times3.25)}}"
            align="center"
            loading="lazy"
          />
        </p>
        <p>
          We interpret this result to mean that the probability of belonging to
          class=1 (short) is 37%. The probability of the observation belonging
          to class=0 (long) is 100%-37% = 63%. Our model predicts that an
          eruption with a duration of 3.25 minutes would belong to the long
          class of eruption.
        </p>
        <h3>Challenge</h3>
        <p>
          Using the same geyser data set in the example above, plot the
          <code>waiting</code>
          column along with the binary class we assigned above
          (<code>kind_binary</code>). Sketch out the shape of the sigmoid
          function that would best fit the data.
        </p>
        <h3>Additional Resources</h3>
        <ul>
          <li>
            <a
              href="https://www.dataschool.io/guide-to-logistic-regression/"
              target="_blank"
              rel="noopener noreferrer"
            >
              Guide to Logistic Regression
            </a>
          </li>
          <li>
            <a
              href="http://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat"
              target="_blank"
              rel="noopener noreferrer"
            >
              Old Faithful Geyser Data
            </a>
          </li>
        </ul>
      </section>

      <section class="content-box">
        <h2>
          Objective 04 - Use scikit learn to fit and interpret logistic
          regression models
        </h2>
        <h3>Overview</h3>
        <p>
          So far, we've looked at the function and coefficients used to fit a
          logistic regression. In this objective we're going to go more into
          detail about how to use the scikit learn <code>LogisticRegression</code> predictor.
          We'll also cover how to fit this model using the two features in the
          dataset and how to interpret these results.
        </p>
        <h3>Follow Along</h3>
        <p>
          Let's load the geyser data set we used earlier, and go through the
          steps to fit a logistic regression model.
        </p>
        <pre><code># Import seaborn and load the data
import seaborn as sns

geyser = sns.load_dataset("geyser")

# Convert target labels to 0 or 1

# Import the label encoder and instantiate
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

# Create a new column with 0=long and 1=short class labels
geyser['kind_binary'] = le.fit_transform(geyser['kind'])
display(geyser.head())
</code></pre>
        <table class="custom-table">
          <thead>
            <tr>
              <th></th>
              <th>duration</th>
              <th>waiting</th>
              <th>kind</th>
              <th>kind_binary</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>0</td>
              <td>3.600</td>
              <td>79</td>
              <td>long</td>
              <td>0</td>
            </tr>
            <tr>
              <td>1</td>
              <td>1.800</td>
              <td>54</td>
              <td>short</td>
              <td>1</td>
            </tr>
            <tr>
              <td>2</td>
              <td>3.333</td>
              <td>74</td>
              <td>long</td>
              <td>0</td>
            </tr>
            <tr>
              <td>3</td>
              <td>2.283</td>
              <td>62</td>
              <td>short</td>
              <td>1</td>
            </tr>
            <tr>
              <td>4</td>
              <td>4.533</td>
              <td>85</td>
              <td>long</td>
              <td>0</td>
            </tr>
          </tbody>
        </table>
        <p>
          Now that we have our geyser class encoded, we can follow the usual
          model fitting procedure. First, we create the feature matrix and
          target array. Then we import the <code>LogisticRegression</code> model, instantiate
          the predictor class, and fit the model.
        </p>
        <pre><code># Import logistic regression predictor
from sklearn.linear_model import LogisticRegression

# Prepare the feature (we'll begin with one feature)
import numpy as np

x = geyser['duration'] 
X = np.array(x)[:, np.newaxis] 

# Assign the target variable to y
y = geyser['kind_binary']

# Fit the model using the default parameters
model = LogisticRegression()
model.fit(X, y)</code></pre>
<pre><code>LogisticRegression()</code></pre>
<pre><code># Import the cross validation method
from sklearn.model_selection import cross_val_score

# Implement a cross-validation with k=5
print(cross_val_score(model, X, y, cv=5))

# Calculate the mean of the cross-validation scores
score_mean = cross_val_score(model, X, y, cv=5).mean()
print('The mean CV score is: ', score_mean)
</code></pre>
        <pre><code>[0.94545455 1.         1.         0.94444444 1.        ]
The mean CV score is:  0.977979797979798
</code></pre>
        <p>
          This model is pretty accurate. If we remember from earlier in this
          module, our baseline accuracy was 63%. We've improved over the
          baseline by a significant amount.
        </p>
        <p>
          Now, how much can we improve our model by using an additional feature
          in fitting our model? We still have the <code>waiting</code> column which is the
          amount of time that passes between eruptions. Let's add that feature
          to the feature matrix, fit the model, and calculate the
          cross-validation score.
        </p>
        <pre><code># Create new feature matrix
features = ['duration', 'waiting']
X_two = geyser[features]

# Fit the model using the default parameters
model_two = LogisticRegression()
model_two.fit(X_two, y)

# Implement a cross-validation with k=5
print(cross_val_score(model_two, X_two, y, cv=5))

# Calculate the mean of the cross-validation scores
score_mean = cross_val_score(model_two, X_two, y, cv=5).mean()
print('The mean CV score is (two features): ', score_mean)
</code></pre>
        <pre><code>[1. 1. 1. 1. 1.]
The mean CV score is (two features):  1.0
</code></pre>
        <p>
          TThe accuracy is perfect for this model. This is likely because the
          two classes have a very clear division. It's important to remember
          that not all data sets will be so easy to model with such accurate
          results!
        </p>
        <h3>Challenge</h3>
        <p>
          For this challenge, try to plot the two features on the same plot. So
          instead of a plot with the feature on the x-axis and the class on the
          y axis, plot one feature on each axis. Are the two classes distinct as
          visualized on the plot? If you use two different colors for the
          classes, there should be a clear division between them. Think about
          where you would draw the decision boundary.
        </p>
        <h3>Additional Resources</h3>
        <ul>
          <li>
            <a
              href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
              target="_blank"
              rel="noopener noreferrer"
              >Scikit learn: Logistic Regression</a
            >
          </li>
        </ul>
      </section>

      <section class="content-box"></section>

      <section class="content-box"></section>

      <section class="content-box"></section>

      <section class="content-box">
        <h2>Guided Project</h2>
        <p>
          Open <strong>JDS_SHR_214_guided_project_notes.ipynb</strong> in the
          GitHub repository below to follow along with the guided project:
        </p>
        <div class="resource-links">
          <a
            href="https://github.com/bloominstituteoftechnology/DS-Unit-2-Linear-Models/tree/master/module4-logistic-regression"
            class="resource-link"
            target="_blank"
            rel="noopener noreferrer"
            >GitHub: Logistic Regression</a
          >
          <a
            href="https://docs.google.com/presentation/d/1VWvH9jKBj63sirqUsQuNxdNme97rEO5uElH8AlBdbcM/present?slide=id.g125f5691cb9_0_0"
            class="resource-link"
            target="_blank"
            rel="noopener noreferrer"
            >Slides</a
          >
        </div>

        <h2>Guided Project Video</h2>
        <div class="video-container">
          <iframe
            class="wistia_embed"
            title="Sprint 5 Logistic Regression Video"
            src="https://fast.wistia.net/embed/iframe/rlh3upen6m"
            width="640"
            height="360"
            allow="fullscreen"
            loading="lazy"
          ></iframe>
        </div>
      </section>

      <section class="content-box">
        <h2>Module Assignment</h2>
        <p>
          Complete the Module 4 assignment to practice logistic regression
          techniques you've learned.
        </p>
        <div class="resource-links">
          <a
            href="https://github.com/bloominstituteoftechnology/DS-Unit-2-Linear-Models/blob/master/module4-logistic-regression/LS_DS_214_assignment.ipynb"
            class="resource-link"
            target="_blank"
            rel="noopener noreferrer"
            >Module 4 Assignment</a
          >
          <a
            href="https://srcole.github.io/100burritos/"
            class="resource-link"
            target="_blank"
            rel="noopener noreferrer"
            >Dataset: Burritos of San Diego</a
          >
        </div>

        <h2>Assignment Solution Video</h2>
        <div class="video-container">
          <iframe
            class="wistia_embed"
            title="Logistic Regression - Module Project Solution Video"
            src="https://fast.wistia.net/embed/iframe/8kq0iynda6"
            width="640"
            height="360"
            allow="fullscreen"
            loading="lazy"
          ></iframe>
        </div>
      </section>

      <section class="content-box">
        <h2>Resources</h2>

        <h3>Documentation and Tutorials</h3>
        <ul>
          <li>
            <a
              href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
              target="_blank"
              rel="noopener noreferrer"
              >Scikit-learn: train_test_split</a
            >
          </li>
          <li>
            <a
              href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
              target="_blank"
              rel="noopener noreferrer"
              >Scikit-learn: LogisticRegression</a
            >
          </li>
          <li>
            <a
              href="https://scikit-learn.org/stable/modules/compose.html#pipeline"
              target="_blank"
              rel="noopener noreferrer"
              >Scikit-learn: Pipelines</a
            >
          </li>
          <li>
            <a
              href="https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics"
              target="_blank"
              rel="noopener noreferrer"
              >Scikit-learn: Classification Metrics</a
            >
          </li>
        </ul>

        <h3>Articles and Readings</h3>
        <ul>
          <li>
            <a
              href="https://machinelearningmastery.com/logistic-regression-for-machine-learning/"
              target="_blank"
              rel="noopener noreferrer"
              >Logistic Regression for Machine Learning</a
            >
          </li>
          <li>
            <a
              href="https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7"
              target="_blank"
              rel="noopener noreferrer"
              >Your validation loss is lower than your training loss? This is
              why!</a
            >
          </li>
        </ul>
      </section>
    </main>
  </body>
</html>
