<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DS7 Module 4 - Model Interpretation</title>
    <link rel="stylesheet" href="../../../css/style.css">
</head>

<body>
    <header>
        <nav>
            <div class="logo">Data Science Unit 2</div>
            <ul>
                <li><a href="../../index.html">Home</a></li>
                <li class="dropdown">
                    <a href="#" class="active">Modules</a>
                    <div class="dropdown-content">
                        <a href="../module1/index.html">Module 1: Define ML Problems</a>
                        <a href="../module2/index.html">Module 2: Wrangle ML Datasets</a>
                        <a href="../module3/index.html">Module 3: Permutation and Boosting</a>
                        <a href="../module4/index.html" class="active">Module 4: Model Interpretation</a>
                    </div>
                </li>
                <li><a href="../../code-alongs/index.html">Code-Alongs</a></li>
                <li><a href="../../sprint-challenge/index.html">Sprint Challenge</a></li>
            </ul>
        </nav>
    </header>

    <main class="container">
        <h1>Module 4: Model Interpretation</h1>

        <section class="content-box">
            <h2>Module Overview</h2>
            <p>In this final module of the sprint, you'll learn techniques for interpreting machine learning models and
                explaining their predictions. Model interpretability is crucial for building stakeholder trust, ensuring
                ethical decision-making, debugging models, and gaining insights into your data that you can communicate
                effectively.</p>
        </section>

        <section class="content-box">
            <h2>Learning Objectives</h2>
            <ul>
                <li>Model Interpretability</li>
                <li>Visualize and interpret PDP plots</li>
                <li>Explain individual predictions with shapley value plots</li>
            </ul>
        </section>

        <section class="content-box">
            <h2>Objective 01 - visualize and interpret partial dependence plots</h2>
            <h3>Overview</h3>
            <p>In this Sprint, we have been focusing on the ranking features by their importance to the model. We've
                looked at lists of features and their contribution to the model. Sometimes, we want more information
                about how the model is working and how a specific feature(s) affects the model predictions. One of the
                main ways to take a look inside a "black box" model is to utilize partial dependence plots.</p>
            <p>A partial dependence plot can be used to show how a model prediction partially depends on values of the
                input variables of interest (the input feature). For example, in the data we'll explore below, we can
                fit a model to predict wine quality. Once we have the model fit, we can isolate a particular feature and
                visualize how the model prediction depends on that feature over its range of values. The plots below
                will show how we visualize the model prediction's dependence on alcohol content.</p>
            <h3>Follow Along</h3>
            <p>We're back to the wine quality data set. In this example, we're going to change our target to having only
                two classes (bad/good): 0 for quality less than or equal to 5 and 1 for above five. Rethinking this
                problem as a binary classification will make it easier to visualize our partial dependence plots because
                the model itself will be simpler.</p>
            <p>After loading in the data, we'll fit a decision tree classifier model. We're not going to split the data
                into training and testing sets because we're not looking at the accuracy or other evaluation metrics
                right now. We'll use all of the data available to train the model and create the partial dependence
                plots.</p>
            <pre>
# Load the dataset
import pandas as pd
wine = pd.read_csv('winequality-red.csv')
wine.head()
fixed acidity	volatile acidity	citric acid	residual sugar	chlorides	free sulfur dioxide	total sulfur dioxide	density	pH	sulphates	alcohol	quality
0	7.4	0.70	0.00	1.9	0.076	11.0	34.0	0.9978	3.51	0.56	9.4	5
1	7.8	0.88	0.00	2.6	0.098	25.0	67.0	0.9968	3.20	0.68	9.8	5
2	7.8	0.76	0.04	2.3	0.092	15.0	54.0	0.9970	3.26	0.65	9.8	5
3	11.2	0.28	0.56	1.9	0.075	17.0	60.0	0.9980	3.16	0.58	9.8	6
4	7.4	0.70	0.00	1.9	0.076	11.0	34.0	0.9978	3.51	0.56	9.4	5
# Set the features list and target variable 
target = 'quality'
X = wine.drop(target, axis=1)

# Create the target array
y = wine['quality']

# Map the target to a binary class at quality = 5
y = y.apply(lambda x: 0 if x <= 5 else 1)

# Instantiate the classifier
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(random_state=42)

wine_model = clf.fit(X, y)
</pre>
            <h3>Calculating the Partial Dependence</h3>
            <p>We always calculate our partial dependence values after we have fit a model. Using the
                PartialDependenceDisplay() module, we specify the features that we would like to visualize. In this
                case, the alcohol and chlorides are the two features we're using. Usually, you would have analyzed the
                feature importances and then selected the most important ones to use. In this example, we're using a
                slightly different red wine data set than what is available from the scikit-learn datasets module, but
                we know that alcohol is one of the top features in terms of importance.</p>
            <pre>
from sklearn.inspection import PartialDependenceDisplay
import matplotlib.pyplot as plt

fig, ax = plt.subplots(1,1, figsize=(12,4))
PartialDependenceDisplay.from_estimator(wine_model, feature_names=X.columns,
    features=['alcohol','chlorides'],
    X=X, grid_resolution=50, ax=ax);

ax.set_title('Wine: Partial Dependence')

plt.show()
</pre>
            <p>&lt;Figure size 864x288 with 0 Axes&gt;<br>
                mod4_obj1_pdp_separate.png</p>
            <p>On the left plot, we can see that as the percentage of alcohol increases the model more strongly predicts
                higher quality. On the right, the amount of chlorides in the wine doesn't seem to affect the prediction
                as much as the alcohol content feature.</p>
            <p>Now, we're going to use a different library to visualize the partial dependence for one feature. This
                plot is calculating the same dependence of the feature but is plotting all of the predictions (shaded
                area) as well as the average (solid line).</p>
            <pre>
!pip install PDPbox

from pdpbox.pdp import PDPIsolate, PDPInteract
from pdpbox.info_plots import TargetPlot, InteractTargetPlot

isolated = PDPIsolate(
    model= wine_model,
    df=X,
    model_features=X.columns,
    feature='alcohol',
    feature_name='alcohol'
)
fig, axes = isolated.plot()
fig
</pre>
            <p>mod4_obj1_pdpbox.png</p>
            <p>This plot is on a different scale that the first one but we can still see the same increase after 11%
                alcohol content.</p>
            <h3>Challenge</h3>
            <p>From the above example, select a different feature (or features) and re-create the plots above. Write out
                a few sentences to describe the dependence of the model prediction on that feature. Is the model
                affected by changes in the feature's values?</p>
            <h3>Additional Resources</h3>
            <ul>
                <li><a href="https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009" target="_blank"
                        rel="noopener noreferrer">Kaggle: Red-wine Quality</a></li>
                <li><a href="https://www.kaggle.com/code/dansbecker/partial-dependence-plots" target="_blank"
                        rel="noopener noreferrer">Kaggle: Partial Dependence Plots</a></li>
                <li><a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_partial_dependence.html"
                        target="_blank" rel="noopener noreferrer">Scikit-learn: plot_partial_dependence</a></li>
                <li><a href="https://github.com/SauceCat/PDPbox" target="_blank" rel="noopener noreferrer">PDPBox</a>
                </li>
            </ul>
        </section>

        <section class="content-box">
            <h2>Objective 02 - explain individual predictions with shapley value plots</h2>
            <h3>Overview</h3>
            <p>In this objective, we'll dive a little deeper into how individual features contribute to making a
                prediction. A quantity called a Shapley value, borrowed from the concept of game theory, will be used to
                explain the difference between the prediction of an individual observation and the average prediction
                from all the observations (instances).</p>
            <p>One way to consider this problem is to think of all features in the model, that are working together to
                make a prediction. For example, using the wine data set from the previous objective, we can consider the
                alcohol content, the sulphates, and the pH. For one observation (row of the DataFrame) they work
                together to make a prediction. The rest of the rows all work together to make the average prediction.
                The Shapley values look at each feature's contribution to the single row prediction and explain exactly
                how it is different from the average prediction.</p>
            <p>Let's work through an example using the SHAP library which stands for SHapley Additive exPlanations.</p>
            <h3>Follow Along</h3>
            <pre>
# Load the dataset
import pandas as pd
wine = pd.read_csv('winequality-red.csv')
wine.head()

# Set the features list and target variable 
target = 'quality'
X = wine.drop(target, axis=1)

# Create the target array
y = wine['quality']

# Map the target to a binary class at quality = 5
y = y.apply(lambda x: 0 if x <= 5 else 1)
            </pre>
            <p>To calculate the Shapley values, we'll be fitting a xgboost classifier model to the data. The
                TreeExplainer() is used to explain the output of ensemble tree models. From the explainer values, we
                calculate the Shapley values and then visualize the results for a single observation (instance).</p>
            <pre>
# import shap 
import shap 

# Instantiate and fit the model 
from xgboost import XGBClassifier 
xgb = XGBClassifier(random_state=42) 
wine_model = xgb.fit(X, y) 

# Shap explainer initilization 
shap_ex = shap.TreeExplainer(wine_model) 

# Determine Shap values 
shap_values = shap_ex.shap_values(X)
import matplotlib.pyplot as plt

# Calculate the shapley values
shap_values = shap_ex.shap_values(X)
  
# Initialize the plot 
shap.initjs() 

shap.force_plot(shap_ex.expected_value, shap_values[25,:], X.iloc[25,:])
            </pre>
            <p>mod4_obj2_SHAP.png</p>
            <p>We can interpret the above plot in the following way: For the sample in 25th row, higher volatile acidity
                helped improve the wine quality, whereas a higher alcohol content and residual sugar (and others)
                resulted in lowering the quality of wine.</p>
            <h3>Challenge</h3>
            <p>We plotted the feature contributions for a single observation. Using the same code as above, try
                selecting a different observation or row of the data frame. You might consider changing the value of 25
                in shap_values[25,:] and X.iloc[25,:]. Write out a few sentences to describe what the SHAP plot
                indicates about the feature contributions for this observation.</p>
            <h3>Additional Resources</h3>
            <ul>
                <li><a href="https://christophm.github.io/interpretable-ml-book/shap.html" target="_blank"
                        rel="noopener noreferrer">Interpretable ML Book: SHAP</a></li>
                <li><a href="https://shap.readthedocs.io/en/latest/index.html" target="_blank"
                        rel="noopener noreferrer">SHAP Library</a></li>
            </ul>
        </section>
        explain the difference between the prediction of an individual observation and the average prediction
        from all the observations (instances).</p>
        </section>

        <section class="content-box">
            <h2>Guided Project</h2>
            <p>Open <strong>DS_234_guided_project_notes.ipynb</strong> in the GitHub repository below to follow along
                with the guided project:</p>
            <div class="resource-links">
                <a href="https://github.com/bloominstituteoftechnology/DS-Unit-2-Applied-Modeling/tree/master/module4-model-interpretation"
                    class="resource-link" target="_blank" rel="noopener noreferrer">GitHub: Model Interpretation</a>
                <a href="https://docs.google.com/presentation/d/19_xQ1B7Bwbb1HEFMGnM7eXiPMogOjhtoLc018G5hnpg/present?slide=id.g125f5691cb9_0_0"
                    class="resource-link" target="_blank" rel="noopener noreferrer">Slides</a>
            </div>

            <h2>Guided Project Video - Part One</h2>
            <div class="video-container">
                <iframe class="wistia_embed" title="Model Interpretation - Part One"
                    src="https://fast.wistia.net/embed/iframe/dstvalsfjl" width="640" height="360" allow="fullscreen"
                    loading="lazy"></iframe>
            </div>

            <h2>Guided Project Video - Part Two</h2>
            <div class="video-container">
                <iframe class="wistia_embed" title="Model Interpretation - Part Two"
                    src="https://fast.wistia.net/embed/iframe/zugvd8mxx2" width="640" height="360" allow="fullscreen"
                    loading="lazy"></iframe>
            </div>
        </section>

        <section class="content-box">
            <h2>Module Assignment</h2>
            <p>For this final assignment, you'll apply model interpretation techniques to your portfolio project to gain
                insights and effectively communicate your model's behavior.</p>

            <p><em>Note: There is no video for this assignment as you will be working with your own dataset and defining
                    your own machine learning problem.</em></p>

            <div class="resource-links">
                <a href="https://github.com/bloominstituteoftechnology/DS-Unit-2-Applied-Modeling/blob/master/module4-model-interpretation/LS_DS_234_assignment.ipynb"
                    class="resource-link" target="_blank" rel="noopener noreferrer">Module 4 Assignment</a>
            </div>
        </section>

        <section class="content-box">
            <h2>Additional Resources</h2>
            <div class="resource-card">
                <h3>Model Validation</h3>
                <ul>
                    <li><a href="https://scikit-learn.org/stable/modules/cross_validation.html" target="_blank"
                            rel="noopener noreferrer">Scikit-learn Cross-validation Guide</a></li>
                    <li><a href="https://scikit-learn.org/stable/modules/learning_curve.html" target="_blank"
                            rel="noopener noreferrer">Validation Curve Documentation</a></li>
                    <li><a href="https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/"
                            target="_blank" rel="noopener noreferrer">Evaluation Metrics for Imbalanced
                            Classification</a></li>
                </ul>
            </div>

            <div class="resource-card">
                <h3>Hyperparameter Tuning</h3>
                <ul>
                    <li><a href="https://scikit-learn.org/stable/modules/grid_search.html" target="_blank"
                            rel="noopener noreferrer">Scikit-learn Tuning Guide for Estimators</a></li>
                    <li><a href="https://github.com/microsoft/nni" target="_blank" rel="noopener noreferrer">Microsoft's
                            Neural
                            Network Intelligence (NNI) Toolkit</a></li>
                    <li><a href="https://optuna.org/" target="_blank" rel="noopener noreferrer">Optuna Hyperparameter
                            Optimization
                            Framework</a></li>
                </ul>
            </div>

            <div class="resource-card">
                <h3>Data Visualization and Communication</h3>
                <ul>
                    <li><a href="https://matplotlib.org/stable/gallery/index.html" target="_blank"
                            rel="noopener noreferrer">Matplotlib Gallery</a></li>
                    <li><a href="https://seaborn.pydata.org/examples/index.html" target="_blank"
                            rel="noopener noreferrer">Seaborn
                            Example Gallery</a></li>
                </ul>
            </div>
        </section>
    </main>
</body>

</html>