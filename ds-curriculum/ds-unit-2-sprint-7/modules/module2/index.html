<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DS7 Module 2 - Wrangle ML Datasets</title>
    <link rel="stylesheet" href="../../../css/style.css">
</head>

<body>
    <header>
        <nav>
            <div class="logo">Data Science Unit 2</div>
            <ul>
                <li><a href="../../index.html">Home</a></li>
                <li class="dropdown">
                    <a href="#" class="active">Modules</a>
                    <div class="dropdown-content">
                        <a href="../module1/index.html">Module 1: Define ML Problems</a>
                        <a href="../module2/index.html" class="active">Module 2: Wrangle ML Datasets</a>
                        <a href="../module3/index.html">Module 3: Permutation and Boosting</a>
                        <a href="../module4/index.html">Module 4: Model Interpretation</a>
                    </div>
                </li>
                <li><a href="../../code-alongs/index.html">Code-Alongs</a></li>
                <li><a href="../../sprint-challenge/index.html">Sprint Challenge</a></li>
            </ul>
        </nav>
    </header>

    <main class="container">
        <h1>Module 2: Wrangle ML Datasets</h1>

        <section class="content-box">
            <h2>Module Overview</h2>
            <p>In this module, you'll learn essential techniques for wrangling datasets for machine learning. Data
                preparation is a critical step in the machine learning workflow, often taking up to 80% of a data
                scientist's time. You'll explore methods for data cleaning, exploration, and joining relational data to
                create meaningful feature sets for your models.</p>
        </section>

        <section class="content-box">
            <h2>Learning Objectives</h2>
            <ul>
                <li>explore tabular data for supervised machine learning</li>
                <li>join relational data for supervised machine learning</li>
            </ul>
        </section>

        <section class="content-box">
            <h2>Objective 01 - explore tabular data for supervised machine learning</h2>
            <h3>Overview</h3>
            <p>In this module, we'll continue with our previous theme of learning to better understand our data in terms
                of preparation for modeling. In this objective, we'll explore a small data set and get some additional
                practice in basic cleaning; identifying columns that might be suitable for feature engineering and
                generally preparing the data for machine learning tasks.</p>
            <h3>Follow Along</h3>
            <p>The data we'll use in the following example is from the USGS Earthquake Catalog and the PNSN Earthquake
                catalog. We'll load in this data and go through a few processing steps to prepare it for some further
                analysis.</p>
            <pre>
# Load in earthquake data sets
import pandas as pd

cols_set1 = ['Evid', 'Magnitude', 'Magnitude Type', 'Time UTC', 'Lat', 'Lon', 'Depth Km']
eq_set1 = pd.read_csv('pnsn_eqlist.csv', usecols=cols_set1)

cols_set2 = ['time', 'latitude', 'longitude', 'depth', 'mag', 'magType', 'net', 'id']
eq_set2 = pd.read_csv('usgs_eqlist.csv', usecols=cols_set2)

display(eq_set1.head())
display(eq_set2.head())
</pre>
            <table>
                <thead>
                    <tr>
                        <th>Evid</th>
                        <th>Magnitude</th>
                        <th>Magnitude Type</th>
                        <th>Time UTC</th>
                        <th>Lat</th>
                        <th>Lon</th>
                        <th>Depth Km</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>0</td>
                        <td>61647142</td>
                        <td>2.5</td>
                        <td>l</td>
                        <td>2020/07/11 00:16:31</td>
                        <td>45.8433</td>
                        <td>-126.3158</td>
                        <td>75.4</td>
                    </tr>
                    <tr>
                        <td>1</td>
                        <td>61656881</td>
                        <td>2.6</td>
                        <td>l</td>
                        <td>2020/07/09 13:46:58</td>
                        <td>42.6240</td>
                        <td>-127.2607</td>
                        <td>26.8</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>61653226</td>
                        <td>2.9</td>
                        <td>l</td>
                        <td>2020/07/04 12:43:18</td>
                        <td>43.0163</td>
                        <td>-127.4387</td>
                        <td>20.8</td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td>61653141</td>
                        <td>2.1</td>
                        <td>l</td>
                        <td>2020/07/04 07:56:07</td>
                        <td>48.8980</td>
                        <td>-121.9658</td>
                        <td>0.6</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>61652566</td>
                        <td>3.3</td>
                        <td>l</td>
                        <td>2020/07/03 13:34:27</td>
                        <td>42.0197</td>
                        <td>-126.3753</td>
                        <td>10.0</td>
                    </tr>
                </tbody>
            </table>
            <table>
                <thead>
                    <tr>
                        <th>time</th>
                        <th>latitude</th>
                        <th>longitude</th>
                        <th>depth</th>
                        <th>mag</th>
                        <th>magType</th>
                        <th>net</th>
                        <th>id</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>0</td>
                        <td>2020-07-11T21:17:57.180Z</td>
                        <td>49.386000</td>
                        <td>-120.536500</td>
                        <td>-0.37</td>
                        <td>2.08</td>
                        <td>ml</td>
                        <td>uw</td>
                        <td>uw61647417</td>
                    </tr>
                    <tr>
                        <td>1</td>
                        <td>2020-07-10T13:38:32.510Z</td>
                        <td>42.317333</td>
                        <td>-121.799000</td>
                        <td>-1.45</td>
                        <td>2.05</td>
                        <td>ml</td>
                        <td>uw</td>
                        <td>uw61646707</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>2020-07-04T12:43:17.982Z</td>
                        <td>43.222800</td>
                        <td>-126.950900</td>
                        <td>10.00</td>
                        <td>2.90</td>
                        <td>ml</td>
                        <td>us</td>
                        <td>us7000ahx7</td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td>2020-07-04T07:56:07.610Z</td>
                        <td>48.895500</td>
                        <td>-121.966833</td>
                        <td>0.30</td>
                        <td>2.14</td>
                        <td>ml</td>
                        <td>uw</td>
                        <td>uw61653141</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>2020-07-03T13:34:25.483Z</td>
                        <td>42.037900</td>
                        <td>-126.077500</td>
                        <td>10.00</td>
                        <td>3.20</td>
                        <td>ml</td>
                        <td>us</td>
                        <td>us7000ahdg</td>
                    </tr>
                </tbody>
            </table>
            <p>We can see that each list of earthquakes has an identification column (Evid and id). If we looked at more
                rows, we would see that there is some overlap. In order to combine these two tables into one, we need to
                do a bit of cleaning so that we can compare the identification columns.</p>
            <pre>
# Clean up eq_set2 id column
# (strip the 'uw' or 'us' from the number)
eq_set2['id'] = eq_set2['id'].map(lambda x: x.lstrip('uws'))
# Add column with the network code

eq_set1['net'] = 'uw'

# Rename columns
new_cols = ['id', 'mag','magType','time','latitude','longitude','depth','net']
eq_set1.columns = new_cols

eq_set1.head()
</pre>
            <table>
                <thead>
                    <tr>
                        <th>id</th>
                        <th>mag</th>
                        <th>magType</th>
                        <th>time</th>
                        <th>latitude</th>
                        <th>longitude</th>
                        <th>depth</th>
                        <th>net</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>0</td>
                        <td>61647142</td>
                        <td>2.5</td>
                        <td>l</td>
                        <td>2020/07/11 00:16:31</td>
                        <td>45.8433</td>
                        <td>-126.3158</td>
                        <td>75.4</td>
                        <td>uw</td>
                    </tr>
                    <tr>
                        <td>1</td>
                        <td>61656881</td>
                        <td>2.6</td>
                        <td>l</td>
                        <td>2020/07/09 13:46:58</td>
                        <td>42.6240</td>
                        <td>-127.2607</td>
                        <td>26.8</td>
                        <td>uw</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>61653226</td>
                        <td>2.9</td>
                        <td>l</td>
                        <td>2020/07/04 12:43:18</td>
                        <td>43.0163</td>
                        <td>-127.4387</td>
                        <td>20.8</td>
                        <td>uw</td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td>61653141</td>
                        <td>2.1</td>
                        <td>l</td>
                        <td>2020/07/04 07:56:07</td>
                        <td>48.8980</td>
                        <td>-121.9658</td>
                        <td>0.6</td>
                        <td>uw</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>61652566</td>
                        <td>3.3</td>
                        <td>l</td>
                        <td>2020/07/03 13:34:27</td>
                        <td>42.0197</td>
                        <td>-126.3753</td>
                        <td>10.0</td>
                        <td>uw</td>
                    </tr>
                </tbody>
            </table>
            <p>We also need to check if the id column is of the same type for both DataFrames and change it if it's not.
            </p>
            <pre>
# Check data types
print('Data type for eq_set1: ', eq_set1['id'].dtype)
print('Data type for eq_set2: ', eq_set2['id'].dtype)

# Change data type
eq_set1['id'] = eq_set1['id'].astype('str')
print('Data type for eq_set1: ', eq_set1['id'].dtype)
</pre>
            <p>Data type for eq_set1: int64<br>
                Data type for eq_set2: object<br>
                Data type for eq_set1: object</p>
            <p>Now we are ready to combine these DataFrames and continue on with our analysis.</p>
            <h3>Challenge</h3>
            <p>Using one of your own data sets, write down a list of what you need to do to the data in order to
                continue on with a machine learning model. Do you need to do extensive cleaning? Group columns with high
                cardinality into a smaller number of classes? Eventually combine the individual files or DataFrames?</p>
            <p>All of these steps are important and often take up most of our time. The modeling process will be faster
                (and more accurate) with properly prepared data.</p>
            <h3>Additional Resources</h3>
            <ul>
                <li><a href="https://earthquake.usgs.gov/" target="_blank" rel="noopener noreferrer">USGS
                        Earthquakes</a></li>
                <li><a href="https://earthquake.usgs.gov/earthquakes/search/" target="_blank"
                        rel="noopener noreferrer">USGS Earthquakes Data</a></li>
                <li><a href="https://pnsn.org/" target="_blank" rel="noopener noreferrer">PNSN Earthquakes</a></li>
                <li><a href="https://pnsn.org/earthquakes/catalogs" target="_blank" rel="noopener noreferrer">PNSN
                        Earthquakes Data</a></li>
            </ul>
        </section>

        <section class="content-box">
            <h2>Objective 02 - join relational data for supervised machine learning</h2>
            <h3>Overview</h3>
            <p>At this point, we have explored the earthquake data set, cleaned it up a little bit, and prepared to
                combine it into one file. In the next section we'll combine the two DataFrames, and in the process
                review some of the techniques we learned previous Sprints.</p>
            <h3>Follow Along</h3>
            <p>In the following section, we'll reproduce the cleaning and preparation process from the previous
                objective and continue on with combining our DataFrames. After they're combined, we'll take a look and
                make sure there are no duplicates.</p>
            <pre>
# Load in earthquake data sets
import pandas as pd

cols_set1 = ['Evid', 'Magnitude', 'Magnitude Type', 'Time UTC', 'Lat', 'Lon', 'Depth Km']
eq_set1 = pd.read_csv('pnsn_eqlist.csv', usecols=cols_set1)

cols_set2 = ['time', 'latitude', 'longitude', 'depth', 'mag', 'magType', 'net', 'id']
eq_set2 = pd.read_csv('usgs_eqlist.csv', usecols=cols_set2)

# Clean up eq_set2 id column
# (strip the 'uw' or 'us' from the number)
eq_set2['id'] = eq_set2['id'].map(lambda x: x.lstrip('uws'))

# Add column with the network code
eq_set1['net'] = 'uw'

# Rename columns
new_cols = ['id', 'mag','magType','time','latitude','longitude','depth','net']
eq_set1.columns = new_cols

# Change data type
eq_set1['id'] = eq_set1['id'].astype('str')

# Take a look at our work
display(eq_set1.head())
display(eq_set2.head())
            </pre>
            <table>
                <thead>
                    <tr>
                        <th>id</th>
                        <th>mag</th>
                        <th>magType</th>
                        <th>time</th>
                        <th>latitude</th>
                        <th>longitude</th>
                        <th>depth</th>
                        <th>net</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>61647142</td>
                        <td>2.5</td>
                        <td>l</td>
                        <td>2020/07/11 00:16:31</td>
                        <td>45.8433</td>
                        <td>-126.3158</td>
                        <td>75.4</td>
                        <td>uw</td>
                    </tr>
                    <tr>
                        <td>61656881</td>
                        <td>2.6</td>
                        <td>l</td>
                        <td>2020/07/09 13:46:58</td>
                        <td>42.6240</td>
                        <td>-127.2607</td>
                        <td>26.8</td>
                        <td>uw</td>
                    </tr>
                    <tr>
                        <td>61653226</td>
                        <td>2.9</td>
                        <td>l</td>
                        <td>2020/07/04 12:43:18</td>
                        <td>43.0163</td>
                        <td>-127.4387</td>
                        <td>20.8</td>
                        <td>uw</td>
                    </tr>
                    <tr>
                        <td>61653141</td>
                        <td>2.1</td>
                        <td>l</td>
                        <td>2020/07/04 07:56:07</td>
                        <td>48.8980</td>
                        <td>-121.9658</td>
                        <td>0.6</td>
                        <td>uw</td>
                    </tr>
                    <tr>
                        <td>61652566</td>
                        <td>3.3</td>
                        <td>l</td>
                        <td>2020/07/03 13:34:27</td>
                        <td>42.0197</td>
                        <td>-126.3753</td>
                        <td>10.0</td>
                        <td>uw</td>
                    </tr>
                </tbody>
            </table>
            <table>
                <thead>
                    <tr>
                        <th>time</th>
                        <th>latitude</th>
                        <th>longitude</th>
                        <th>depth</th>
                        <th>mag</th>
                        <th>magType</th>
                        <th>net</th>
                        <th>id</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>2020-07-11T21:17:57.180Z</td>
                        <td>49.386000</td>
                        <td>-120.536500</td>
                        <td>-0.37</td>
                        <td>2.08</td>
                        <td>ml</td>
                        <td>uw</td>
                        <td>61647417</td>
                    </tr>
                    <tr>
                        <td>2020-07-10T13:38:32.510Z</td>
                        <td>42.317333</td>
                        <td>-121.799000</td>
                        <td>-1.45</td>
                        <td>2.05</td>
                        <td>ml</td>
                        <td>uw</td>
                        <td>61646707</td>
                    </tr>
                    <tr>
                        <td>2020-07-04T12:43:17.982Z</td>
                        <td>43.222800</td>
                        <td>-126.950900</td>
                        <td>10.00</td>
                        <td>2.90</td>
                        <td>ml</td>
                        <td>us</td>
                        <td>7000ahx7</td>
                    </tr>
                    <tr>
                        <td>2020-07-04T07:56:07.610Z</td>
                        <td>48.895500</td>
                        <td>-121.966833</td>
                        <td>0.30</td>
                        <td>2.14</td>
                        <td>ml</td>
                        <td>uw</td>
                        <td>61653141</td>
                    </tr>
                    <tr>
                        <td>2020-07-03T13:34:25.483Z</td>
                        <td>42.037900</td>
                        <td>-126.077500</td>
                        <td>10.00</td>
                        <td>3.20</td>
                        <td>ml</td>
                        <td>us</td>
                        <td>7000ahdg</td>
                    </tr>
                </tbody>
            </table>
            <p>Now we can combine the two sets. First, we need to think about how to join the DataFrames. In general, we
                should keep all of the data if possible. In this case, the columns in each DataFrame are the same, so
                we're essentially concatenating the two DataFrames. After we do that, we'll check for duplication in the
                event id and remove any duplicates.</p>
            <pre>
# Concatenate eq_set1 and eq_set2
eq_set_all = pd.concat([eq_set1, eq_set2])
print('Size of DataFrame before dropping duplicates: ', eq_set_all.shape)

# Remove duplicate event id numbers
eq_set_all.drop_duplicates(subset='id', inplace=True)
print('Size of DataFrame after dropping duplicates: ', eq_set_all.shape)
            </pre>
            <p>Size of DataFrame before dropping duplicates: (68, 8)<br>
                Size of DataFrame after dropping duplicates: (50, 8)</p>
            <h3>Challenge</h3>
            <p>It's your turn to find some data that you need to combine or join in some way. Dig up two (or more) data
                sets and identify what you would need to do to combine them into one DataFrame. For more of a challenge,
                you can load, clean, and combine your data set. To make this a little easier, you could use a small
                subset of each to practice on.</p>
            <h3>Additional Resources</h3>
            <ul>
                <li><a href="https://earthquake.usgs.gov/" target="_blank" rel="noopener noreferrer">USGS
                        Earthquakes</a></li>
                <li><a href="https://earthquake.usgs.gov/earthquakes/search/" target="_blank"
                        rel="noopener noreferrer">USGS Earthquakes Data</a></li>
                <li><a href="https://pnsn.org/" target="_blank" rel="noopener noreferrer">PNSN Earthquakes</a></li>
                <li><a href="https://pnsn.org/earthquakes/catalogs" target="_blank" rel="noopener noreferrer">PNSN
                        Earthquakes Data</a></li>
            </ul>
        </section>

        <section class="content-box">
            <h2>Guided Project</h2>
            <p>Open <strong>DS_232_guided_project.ipynb</strong> in the GitHub repository below to follow along with the
                guided project:</p>
            <div class="resource-links">
                <a href="https://github.com/bloominstituteoftechnology/DS-Unit-2-Applied-Modeling/tree/master/module2-wrangle-ml-datasets"
                    class="resource-link" target="_blank" rel="noopener noreferrer">GitHub: Wrangle ML Datasets</a>
                <a href="https://docs.google.com/presentation/d/19jJKZAcpXJeudTFx4HCdfyI5i1C7l-ouaFXDYM1hHj4/present?slide=id.g125f5691cb9_0_0"
                    class="resource-link" target="_blank" rel="noopener noreferrer">Slides</a>
            </div>

            <h2>Guided Project Video</h2>
            <div class="video-container">
                <iframe class="wistia_embed" title="Wrangle ML Datasets Guided Project"
                    src="https://fast.wistia.net/embed/iframe/p24kq8nawt" width="640" height="360" allow="fullscreen"
                    loading="lazy"></iframe>
            </div>
        </section>

        <section class="content-box">
            <h2>Module Assignment</h2>
            <p>For this assignment, you'll continue working with your portfolio dataset from Module 1. You'll apply what
                you've learned to clean, explore, and prepare your data for modeling.</p>

            <p><em>Note: There is no video for this assignment as you will be working with your own dataset and defining
                    your own machine learning problem.</em></p>

            <div class="resource-links">
                <a href="https://github.com/bloominstituteoftechnology/DS-Unit-2-Applied-Modeling/blob/master/module2-wrangle-ml-datasets/LS_DS_232_assignment.ipynb"
                    class="resource-link" target="_blank" rel="noopener noreferrer">Module 2 Assignment</a>
            </div>
        </section>

        <section class="content-box">
            <h2>Additional Resources</h2>
            <div class="resource-card">
                <h3>Data Exploration and Visualization</h3>
                <ul>
                    <li><a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html"
                            target="_blank" rel="noopener noreferrer">Pandas Chart Visualization Guide</a></li>
                    <li><a href="https://seaborn.pydata.org/tutorial.html" target="_blank"
                            rel="noopener noreferrer">Seaborn Tutorials</a></li>
                    <li><a href="https://github.com/ResidentMario/missingno" target="_blank"
                            rel="noopener noreferrer">Missingno - Visualizing Missing Data</a></li>
                </ul>
            </div>

            <div class="resource-card">
                <h3>Data Cleaning and Preparation</h3>
                <ul>
                    <li><a href="https://scikit-learn.org/stable/modules/preprocessing.html" target="_blank"
                            rel="noopener noreferrer">Scikit-learn: Preprocessing Guide</a></li>
                    <li><a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html" target="_blank"
                            rel="noopener noreferrer">Pandas Merging Guide</a></li>
                </ul>
            </div>
        </section>
    </main>
</body>

</html>